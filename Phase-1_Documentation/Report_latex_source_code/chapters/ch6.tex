\chapter{Conclusions and Discussion}\label{final}

\section{Conclusion}

In conclusion we see that the best performing model among all the models and different architectures is the first architecture from RNN models when SVD is applied for selecting 10 components from the dataset features after oversampling and smote balancing the data. Also a clear observation is that the ANN model performed the least compared to the other models. The remaining 4 models gave results almost close to each other. We also see that in some cases, we got NAN values for FPA and CLC but the test loss in those cases is the second least and the train loss was the highest among all the experiments. Hence taking FPA and CLC as the criteria for evalutaing the model performance was better. 

\section{Future scope}
For even more improved results, attention layers can be introduced in the model architectures. Even after taking such complex architectures the results were very close to each other and the dataset was small because of taking cross versions so we had to perform oversampling and SMOTE balancing. It is possible that due to limited data of cross versions for the training of the deep learning models, the results were saturated. Different classical Machine learning techniques could be applied on the same data and checked for better results.