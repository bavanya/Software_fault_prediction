{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MID 11\n",
    "### Taking ant1.3 as training data.\n",
    "### ant1.4 as testing data.\n",
    "### Min max scaling done to few columns: ['wmc', 'dit', 'noc', 'cbo', 'rfc', 'lcom', 'ca', 'ce', 'npm', 'lcom3', 'loc', 'dam', 'moa', 'mfa', 'cam', 'ic', 'cbm', 'amc', 'max_cc', 'avg_cc']\n",
    "### SVD used for feature reduction, n_components = 10.\n",
    "### Oversampling and smote methods used to increase size of training data.\n",
    "### RNN model used, model type is 4 as per the BTP documentation spreadsheet.\n",
    "### np.rint() used on predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time \n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from keras.layers import SimpleRNN, Dense\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data. \n",
    "train_data_path = \"/home/bavanya/Desktop/6thSem/BTP/regression_PROMISE_dataset/datasets/ant-1.3.csv\"\n",
    "test_data_path = \"/home/bavanya/Desktop/6thSem/BTP/regression_PROMISE_dataset/datasets/ant-1.4.csv\"\n",
    "train_data_name = \"ant-1.3\"\n",
    "test_data_name = \"ant-1.4\"\n",
    "ant_1_3 = pd.read_csv(train_data_path)\n",
    "ant_1_4 = pd.read_csv(test_data_path)\n",
    "files = [\"/home/bavanya/Desktop/6thSem/BTP/regression_PROMISE_dataset/datasets/ant-1.3.csv\", \"/home/bavanya/Desktop/6thSem/BTP/regression_PROMISE_dataset/datasets/ant-1.4.csv\"]\n",
    "combined_data = pd.concat(map(pd.read_csv, files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Min Max Scaling.\n",
    "scaler = MinMaxScaler()\n",
    "MinMaxScaler(copy=True, feature_range=(0, 1))\n",
    "cols_to_norm = ['wmc', 'dit', 'noc', 'cbo', 'rfc', 'lcom', 'ca', 'ce', 'npm', 'lcom3', 'loc', 'dam', 'moa', 'mfa', 'cam', 'ic', 'cbm', 'amc', 'max_cc', 'avg_cc']\n",
    "combined_data[cols_to_norm] = MinMaxScaler().fit_transform(combined_data[cols_to_norm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>version</th>\n",
       "      <th>name.1</th>\n",
       "      <th>wmc</th>\n",
       "      <th>dit</th>\n",
       "      <th>noc</th>\n",
       "      <th>cbo</th>\n",
       "      <th>rfc</th>\n",
       "      <th>lcom</th>\n",
       "      <th>ca</th>\n",
       "      <th>...</th>\n",
       "      <th>dam</th>\n",
       "      <th>moa</th>\n",
       "      <th>mfa</th>\n",
       "      <th>cam</th>\n",
       "      <th>ic</th>\n",
       "      <th>cbm</th>\n",
       "      <th>amc</th>\n",
       "      <th>max_cc</th>\n",
       "      <th>avg_cc</th>\n",
       "      <th>bug</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ant</td>\n",
       "      <td>1.3</td>\n",
       "      <td>org.apache.tools.ant.taskdefs.ExecuteOn</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.102941</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.011856</td>\n",
       "      <td>0.014815</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.885057</td>\n",
       "      <td>0.232323</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.165951</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>0.209085</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ant</td>\n",
       "      <td>1.3</td>\n",
       "      <td>org.apache.tools.ant.DefaultLogger</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.163265</td>\n",
       "      <td>0.020033</td>\n",
       "      <td>0.029630</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080979</td>\n",
       "      <td>0.171429</td>\n",
       "      <td>0.269903</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ant</td>\n",
       "      <td>1.3</td>\n",
       "      <td>org.apache.tools.ant.taskdefs.TaskOutputStream</td>\n",
       "      <td>0.038961</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.007353</td>\n",
       "      <td>0.045918</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.083267</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.109529</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ant</td>\n",
       "      <td>1.3</td>\n",
       "      <td>org.apache.tools.ant.taskdefs.Cvs</td>\n",
       "      <td>0.155844</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.088235</td>\n",
       "      <td>0.188776</td>\n",
       "      <td>0.013083</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115693</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>0.232742</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ant</td>\n",
       "      <td>1.3</td>\n",
       "      <td>org.apache.tools.ant.taskdefs.Copyfile</td>\n",
       "      <td>0.077922</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.880952</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.100881</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.136898</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>ant</td>\n",
       "      <td>1.4</td>\n",
       "      <td>org.apache.tools.ant.TaskAdapter</td>\n",
       "      <td>0.064935</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.051471</td>\n",
       "      <td>0.096939</td>\n",
       "      <td>0.001635</td>\n",
       "      <td>0.029630</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.138351</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.131428</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>ant</td>\n",
       "      <td>1.4</td>\n",
       "      <td>org.apache.tools.ant.taskdefs.rmic.DefaultRmic...</td>\n",
       "      <td>0.220779</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.110294</td>\n",
       "      <td>0.336735</td>\n",
       "      <td>0.036795</td>\n",
       "      <td>0.029630</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.197917</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.172373</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.396221</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>ant</td>\n",
       "      <td>1.4</td>\n",
       "      <td>org.apache.tools.ant.IntrospectionHelper</td>\n",
       "      <td>0.298701</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.227941</td>\n",
       "      <td>0.418367</td>\n",
       "      <td>0.082175</td>\n",
       "      <td>0.162963</td>\n",
       "      <td>...</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.219724</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.378561</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>ant</td>\n",
       "      <td>1.4</td>\n",
       "      <td>org.apache.tools.ant.taskdefs.compilers.Defaul...</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.161765</td>\n",
       "      <td>0.438776</td>\n",
       "      <td>0.006132</td>\n",
       "      <td>0.059259</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.373390</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.761689</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>ant</td>\n",
       "      <td>1.4</td>\n",
       "      <td>org.apache.tools.ant.NoBannerLogger</td>\n",
       "      <td>0.051948</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.022059</td>\n",
       "      <td>0.081633</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.064852</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.287498</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>303 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    name  version                                             name.1  \\\n",
       "0    ant      1.3            org.apache.tools.ant.taskdefs.ExecuteOn   \n",
       "1    ant      1.3                 org.apache.tools.ant.DefaultLogger   \n",
       "2    ant      1.3     org.apache.tools.ant.taskdefs.TaskOutputStream   \n",
       "3    ant      1.3                  org.apache.tools.ant.taskdefs.Cvs   \n",
       "4    ant      1.3             org.apache.tools.ant.taskdefs.Copyfile   \n",
       "..   ...      ...                                                ...   \n",
       "173  ant      1.4                   org.apache.tools.ant.TaskAdapter   \n",
       "174  ant      1.4  org.apache.tools.ant.taskdefs.rmic.DefaultRmic...   \n",
       "175  ant      1.4           org.apache.tools.ant.IntrospectionHelper   \n",
       "176  ant      1.4  org.apache.tools.ant.taskdefs.compilers.Defaul...   \n",
       "177  ant      1.4                org.apache.tools.ant.NoBannerLogger   \n",
       "\n",
       "          wmc  dit    noc       cbo       rfc      lcom        ca  ...  \\\n",
       "0    0.142857  0.6  0.050  0.102941  0.214286  0.011856  0.014815  ...   \n",
       "1    0.181818  0.0  0.025  0.058824  0.163265  0.020033  0.029630  ...   \n",
       "2    0.038961  0.2  0.000  0.007353  0.045918  0.000000  0.000000  ...   \n",
       "3    0.155844  0.4  0.000  0.088235  0.188776  0.013083  0.000000  ...   \n",
       "4    0.077922  0.4  0.000  0.029412  0.107143  0.000409  0.000000  ...   \n",
       "..        ...  ...    ...       ...       ...       ...       ...  ...   \n",
       "173  0.064935  0.4  0.000  0.051471  0.096939  0.001635  0.029630  ...   \n",
       "174  0.220779  0.0  0.075  0.110294  0.336735  0.036795  0.029630  ...   \n",
       "175  0.298701  0.0  0.000  0.227941  0.418367  0.082175  0.162963  ...   \n",
       "176  0.142857  0.0  0.200  0.161765  0.438776  0.006132  0.059259  ...   \n",
       "177  0.051948  0.2  0.000  0.022059  0.081633  0.000000  0.000000  ...   \n",
       "\n",
       "          dam       moa       mfa       cam    ic       cbm       amc  \\\n",
       "0    1.000000  0.111111  0.885057  0.232323  0.75  0.363636  0.165951   \n",
       "1    1.000000  0.000000  0.000000  0.307692  0.00  0.000000  0.080979   \n",
       "2    1.000000  0.111111  0.714286  0.666667  0.25  0.090909  0.083267   \n",
       "3    1.000000  0.111111  0.770833  0.458333  0.00  0.000000  0.115693   \n",
       "4    1.000000  0.000000  0.880952  0.416667  0.50  0.181818  0.100881   \n",
       "..        ...       ...       ...       ...   ...       ...       ...   \n",
       "173  0.000000  0.000000  0.902439  0.400000  0.25  0.090909  0.138351   \n",
       "174  1.000000  0.222222  0.000000  0.197917  0.00  0.000000  0.172373   \n",
       "175  0.444444  0.000000  0.000000  0.318182  0.00  0.000000  0.219724   \n",
       "176  1.000000  0.777778  0.000000  0.266667  0.00  0.000000  0.373390   \n",
       "177  1.000000  0.000000  0.842105  0.875000  0.25  0.090909  0.064852   \n",
       "\n",
       "       max_cc    avg_cc  bug  \n",
       "0    0.085714  0.209085    0  \n",
       "1    0.171429  0.269903    2  \n",
       "2    0.028571  0.109529    0  \n",
       "3    0.085714  0.232742    0  \n",
       "4    0.028571  0.136898    0  \n",
       "..        ...       ...  ...  \n",
       "173  0.028571  0.131428    0  \n",
       "174  0.400000  0.396221    0  \n",
       "175  0.685714  0.378561    0  \n",
       "176  0.628571  0.761689    1  \n",
       "177  0.142857  0.287498    0  \n",
       "\n",
       "[303 rows x 24 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define transform\n",
    "components = 10\n",
    "svd = TruncatedSVD(n_components=components)\n",
    "# prepare transform on dataset\n",
    "svd.fit(combined_data[cols_to_norm])\n",
    "# apply transform to dataset\n",
    "transformed = svd.transform(combined_data[cols_to_norm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.697270</td>\n",
       "      <td>-0.240716</td>\n",
       "      <td>0.571264</td>\n",
       "      <td>0.167873</td>\n",
       "      <td>-0.202270</td>\n",
       "      <td>-0.188764</td>\n",
       "      <td>-0.085932</td>\n",
       "      <td>0.036019</td>\n",
       "      <td>-0.075786</td>\n",
       "      <td>0.129869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.016611</td>\n",
       "      <td>-0.267720</td>\n",
       "      <td>-0.587890</td>\n",
       "      <td>-0.054598</td>\n",
       "      <td>-0.014350</td>\n",
       "      <td>-0.077750</td>\n",
       "      <td>-0.160155</td>\n",
       "      <td>0.052980</td>\n",
       "      <td>0.100634</td>\n",
       "      <td>-0.030916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.298451</td>\n",
       "      <td>-0.173527</td>\n",
       "      <td>0.139837</td>\n",
       "      <td>-0.541003</td>\n",
       "      <td>-0.007964</td>\n",
       "      <td>0.011583</td>\n",
       "      <td>0.156168</td>\n",
       "      <td>-0.110641</td>\n",
       "      <td>-0.028186</td>\n",
       "      <td>0.008754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.537891</td>\n",
       "      <td>-0.056428</td>\n",
       "      <td>0.028502</td>\n",
       "      <td>-0.104266</td>\n",
       "      <td>0.280907</td>\n",
       "      <td>0.130204</td>\n",
       "      <td>-0.176106</td>\n",
       "      <td>-0.029262</td>\n",
       "      <td>-0.088801</td>\n",
       "      <td>-0.025663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.530257</td>\n",
       "      <td>-0.074334</td>\n",
       "      <td>0.374525</td>\n",
       "      <td>-0.205480</td>\n",
       "      <td>-0.061914</td>\n",
       "      <td>-0.152907</td>\n",
       "      <td>-0.063597</td>\n",
       "      <td>0.021766</td>\n",
       "      <td>0.089365</td>\n",
       "      <td>0.037752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>0.886501</td>\n",
       "      <td>0.524176</td>\n",
       "      <td>0.551332</td>\n",
       "      <td>0.103135</td>\n",
       "      <td>0.083555</td>\n",
       "      <td>0.172359</td>\n",
       "      <td>0.095620</td>\n",
       "      <td>0.012480</td>\n",
       "      <td>0.004141</td>\n",
       "      <td>0.017085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>1.128822</td>\n",
       "      <td>-0.426056</td>\n",
       "      <td>-0.629426</td>\n",
       "      <td>0.258850</td>\n",
       "      <td>-0.034627</td>\n",
       "      <td>0.177394</td>\n",
       "      <td>-0.220035</td>\n",
       "      <td>0.025682</td>\n",
       "      <td>-0.128906</td>\n",
       "      <td>0.061409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>1.027511</td>\n",
       "      <td>-0.126484</td>\n",
       "      <td>-0.591917</td>\n",
       "      <td>0.769665</td>\n",
       "      <td>0.027848</td>\n",
       "      <td>0.483424</td>\n",
       "      <td>0.049615</td>\n",
       "      <td>-0.134999</td>\n",
       "      <td>-0.142810</td>\n",
       "      <td>0.233455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>1.318002</td>\n",
       "      <td>-0.514244</td>\n",
       "      <td>-0.775534</td>\n",
       "      <td>0.478137</td>\n",
       "      <td>-0.191742</td>\n",
       "      <td>0.531927</td>\n",
       "      <td>-0.176518</td>\n",
       "      <td>0.103397</td>\n",
       "      <td>-0.589727</td>\n",
       "      <td>0.040915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>1.534499</td>\n",
       "      <td>0.032362</td>\n",
       "      <td>0.068811</td>\n",
       "      <td>-0.514331</td>\n",
       "      <td>-0.062047</td>\n",
       "      <td>0.142505</td>\n",
       "      <td>0.205071</td>\n",
       "      <td>-0.085574</td>\n",
       "      <td>0.014607</td>\n",
       "      <td>-0.033795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>303 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0    1.697270 -0.240716  0.571264  0.167873 -0.202270 -0.188764 -0.085932   \n",
       "1    1.016611 -0.267720 -0.587890 -0.054598 -0.014350 -0.077750 -0.160155   \n",
       "2    1.298451 -0.173527  0.139837 -0.541003 -0.007964  0.011583  0.156168   \n",
       "3    1.537891 -0.056428  0.028502 -0.104266  0.280907  0.130204 -0.176106   \n",
       "4    1.530257 -0.074334  0.374525 -0.205480 -0.061914 -0.152907 -0.063597   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "298  0.886501  0.524176  0.551332  0.103135  0.083555  0.172359  0.095620   \n",
       "299  1.128822 -0.426056 -0.629426  0.258850 -0.034627  0.177394 -0.220035   \n",
       "300  1.027511 -0.126484 -0.591917  0.769665  0.027848  0.483424  0.049615   \n",
       "301  1.318002 -0.514244 -0.775534  0.478137 -0.191742  0.531927 -0.176518   \n",
       "302  1.534499  0.032362  0.068811 -0.514331 -0.062047  0.142505  0.205071   \n",
       "\n",
       "            7         8         9  \n",
       "0    0.036019 -0.075786  0.129869  \n",
       "1    0.052980  0.100634 -0.030916  \n",
       "2   -0.110641 -0.028186  0.008754  \n",
       "3   -0.029262 -0.088801 -0.025663  \n",
       "4    0.021766  0.089365  0.037752  \n",
       "..        ...       ...       ...  \n",
       "298  0.012480  0.004141  0.017085  \n",
       "299  0.025682 -0.128906  0.061409  \n",
       "300 -0.134999 -0.142810  0.233455  \n",
       "301  0.103397 -0.589727  0.040915  \n",
       "302 -0.085574  0.014607 -0.033795  \n",
       "\n",
       "[303 rows x 10 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed = pd.DataFrame(transformed)\n",
    "transformed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_index_list = list(range(ant_1_3.shape[0]))\n",
    "train_data_index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 200,\n",
       " 201,\n",
       " 202,\n",
       " 203,\n",
       " 204,\n",
       " 205,\n",
       " 206,\n",
       " 207,\n",
       " 208,\n",
       " 209,\n",
       " 210,\n",
       " 211,\n",
       " 212,\n",
       " 213,\n",
       " 214,\n",
       " 215,\n",
       " 216,\n",
       " 217,\n",
       " 218,\n",
       " 219,\n",
       " 220,\n",
       " 221,\n",
       " 222,\n",
       " 223,\n",
       " 224,\n",
       " 225,\n",
       " 226,\n",
       " 227,\n",
       " 228,\n",
       " 229,\n",
       " 230,\n",
       " 231,\n",
       " 232,\n",
       " 233,\n",
       " 234,\n",
       " 235,\n",
       " 236,\n",
       " 237,\n",
       " 238,\n",
       " 239,\n",
       " 240,\n",
       " 241,\n",
       " 242,\n",
       " 243,\n",
       " 244,\n",
       " 245,\n",
       " 246,\n",
       " 247,\n",
       " 248,\n",
       " 249,\n",
       " 250,\n",
       " 251,\n",
       " 252,\n",
       " 253,\n",
       " 254,\n",
       " 255,\n",
       " 256,\n",
       " 257,\n",
       " 258,\n",
       " 259,\n",
       " 260,\n",
       " 261,\n",
       " 262,\n",
       " 263,\n",
       " 264,\n",
       " 265,\n",
       " 266,\n",
       " 267,\n",
       " 268,\n",
       " 269,\n",
       " 270,\n",
       " 271,\n",
       " 272,\n",
       " 273,\n",
       " 274,\n",
       " 275,\n",
       " 276,\n",
       " 277,\n",
       " 278,\n",
       " 279,\n",
       " 280,\n",
       " 281,\n",
       " 282,\n",
       " 283,\n",
       " 284,\n",
       " 285,\n",
       " 286,\n",
       " 287,\n",
       " 288,\n",
       " 289,\n",
       " 290,\n",
       " 291,\n",
       " 292,\n",
       " 293,\n",
       " 294,\n",
       " 295,\n",
       " 296,\n",
       " 297,\n",
       " 298,\n",
       " 299,\n",
       " 300,\n",
       " 301,\n",
       " 302]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_index_list = list(range(ant_1_3.shape[0], ant_1_3.shape[0] + ant_1_4.shape[0]))\n",
    "test_data_index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = transformed[transformed.index.isin(train_data_index_list)]\n",
    "X_train = np.array(X_train)\n",
    "\n",
    "X_test = transformed[transformed.index.isin(test_data_index_list)]\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = combined_data[transformed.index.isin(train_data_index_list)]\n",
    "Y_train = Y_train['bug']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = combined_data[transformed.index.isin(test_data_index_list)]\n",
    "Y_test = Y_test['bug']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying over sampling and SMOTE to training data for augmentation.\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "X_train, Y_train = ros.fit_resample(X_train, Y_train)\n",
    "\n",
    "smt = SMOTE()\n",
    "X_train, Y_train = smt.fit_resample(X_train, Y_train)\n",
    "\n",
    "train_x = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "test_x = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.69726979, -0.24071623,  0.57126395, ...,  0.03601945,\n",
       "         -0.07578644,  0.1298695 ]],\n",
       "\n",
       "       [[ 1.01661138, -0.26772002, -0.58788997, ...,  0.05298001,\n",
       "          0.10063434, -0.03091627]],\n",
       "\n",
       "       [[ 1.2984506 , -0.17352675,  0.13983707, ..., -0.1106405 ,\n",
       "         -0.02818633,  0.00875374]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 1.01420045, -0.37421302, -0.54721556, ...,  0.0060236 ,\n",
       "          0.0477774 ,  0.1532802 ]],\n",
       "\n",
       "       [[ 1.01420045, -0.37421302, -0.54721556, ...,  0.0060236 ,\n",
       "          0.0477774 ,  0.1532802 ]],\n",
       "\n",
       "       [[ 1.53857934, -0.22516725,  0.44500853, ...,  0.01491268,\n",
       "         -0.05165907,  0.11134909]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      2\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "      ..\n",
       "415    3\n",
       "416    3\n",
       "417    3\n",
       "418    3\n",
       "419    3\n",
       "Name: bug, Length: 420, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 3, 0, 0, 0, 2, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = Y_train.to_numpy()\n",
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.52329275e+00, -1.70090313e-01,  1.28455989e-01, ...,\n",
       "         -6.80751641e-04, -8.03932395e-02, -4.73045611e-02]],\n",
       "\n",
       "       [[ 1.65167769e+00, -6.32547908e-02,  3.68564825e-01, ...,\n",
       "         -8.72648436e-02,  3.00134262e-02,  2.13725871e-01]],\n",
       "\n",
       "       [[ 1.02228015e+00, -1.47043172e-01, -5.94705710e-01, ...,\n",
       "         -7.07956961e-02,  6.31230009e-02, -9.62148357e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 1.02751126e+00, -1.26484498e-01, -5.91917396e-01, ...,\n",
       "         -1.34998564e-01, -1.42810088e-01,  2.33454670e-01]],\n",
       "\n",
       "       [[ 1.31800237e+00, -5.14243570e-01, -7.75534220e-01, ...,\n",
       "          1.03397226e-01, -5.89726796e-01,  4.09146172e-02]],\n",
       "\n",
       "       [[ 1.53449933e+00,  3.23618749e-02,  6.88108962e-02, ...,\n",
       "         -8.55739502e-02,  1.46074654e-02, -3.37952553e-02]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "      ..\n",
       "173    0\n",
       "174    0\n",
       "175    0\n",
       "176    1\n",
       "177    0\n",
       "Name: bug, Length: 178, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "       0, 3, 0, 0, 0, 0, 3, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,\n",
       "       0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y = Y_test.to_numpy()\n",
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Designing and initializing the model.\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(100, input_shape = (1,components), dropout = 0.2, return_sequences=True))\n",
    "model.add(SimpleRNN(80, dropout = 0.2, return_sequences=True))\n",
    "model.add(SimpleRNN(60, dropout = 0.2, return_sequences=False))\n",
    "model.add(Dense(1, activation = 'relu'))\n",
    "model.compile(loss = 'mse' , optimizer = 'adam' , metrics = ['mse', 'mae', tf.keras.metrics.RootMeanSquaredError(), tf.keras.metrics.MeanSquaredLogarithmicError()] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn (SimpleRNN)       (None, 1, 100)            11100     \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 1, 80)             14480     \n",
      "_________________________________________________________________\n",
      "simple_rnn_2 (SimpleRNN)     (None, 60)                8460      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 61        \n",
      "=================================================================\n",
      "Total params: 34,101\n",
      "Trainable params: 34,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 2.3682 - mse: 2.3682 - mae: 1.2515 - root_mean_squared_error: 1.5389 - mean_squared_logarithmic_error: 0.5109\n",
      "Epoch 2/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 1.5290 - mse: 1.5290 - mae: 1.0143 - root_mean_squared_error: 1.2365 - mean_squared_logarithmic_error: 0.3441\n",
      "Epoch 3/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.7054 - mse: 1.7054 - mae: 1.0332 - root_mean_squared_error: 1.3059 - mean_squared_logarithmic_error: 0.3650\n",
      "Epoch 4/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.2928 - mse: 1.2928 - mae: 0.9563 - root_mean_squared_error: 1.1370 - mean_squared_logarithmic_error: 0.2858\n",
      "Epoch 5/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.2788 - mse: 1.2788 - mae: 0.9543 - root_mean_squared_error: 1.1308 - mean_squared_logarithmic_error: 0.2564\n",
      "Epoch 6/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.1745 - mse: 1.1745 - mae: 0.9103 - root_mean_squared_error: 1.0837 - mean_squared_logarithmic_error: 0.2320\n",
      "Epoch 7/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 1.1181 - mse: 1.1181 - mae: 0.8830 - root_mean_squared_error: 1.0574 - mean_squared_logarithmic_error: 0.2238\n",
      "Epoch 8/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.1202 - mse: 1.1202 - mae: 0.8887 - root_mean_squared_error: 1.0584 - mean_squared_logarithmic_error: 0.2384\n",
      "Epoch 9/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.0535 - mse: 1.0535 - mae: 0.8590 - root_mean_squared_error: 1.0264 - mean_squared_logarithmic_error: 0.2277\n",
      "Epoch 10/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 1.0579 - mse: 1.0579 - mae: 0.8548 - root_mean_squared_error: 1.0285 - mean_squared_logarithmic_error: 0.2231\n",
      "Epoch 11/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.9497 - mse: 0.9497 - mae: 0.8225 - root_mean_squared_error: 0.9745 - mean_squared_logarithmic_error: 0.1966\n",
      "Epoch 12/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.9315 - mse: 0.9315 - mae: 0.8085 - root_mean_squared_error: 0.9651 - mean_squared_logarithmic_error: 0.2035\n",
      "Epoch 13/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.9974 - mse: 0.9974 - mae: 0.8221 - root_mean_squared_error: 0.9987 - mean_squared_logarithmic_error: 0.2228\n",
      "Epoch 14/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.9862 - mse: 0.9862 - mae: 0.8215 - root_mean_squared_error: 0.9931 - mean_squared_logarithmic_error: 0.2218\n",
      "Epoch 15/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.0245 - mse: 1.0245 - mae: 0.8518 - root_mean_squared_error: 1.0122 - mean_squared_logarithmic_error: 0.2179\n",
      "Epoch 16/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.9123 - mse: 0.9123 - mae: 0.8075 - root_mean_squared_error: 0.9551 - mean_squared_logarithmic_error: 0.2013\n",
      "Epoch 17/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.8878 - mse: 0.8878 - mae: 0.8024 - root_mean_squared_error: 0.9422 - mean_squared_logarithmic_error: 0.1973\n",
      "Epoch 18/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.9547 - mse: 0.9547 - mae: 0.8097 - root_mean_squared_error: 0.9771 - mean_squared_logarithmic_error: 0.2144\n",
      "Epoch 19/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.9051 - mse: 0.9051 - mae: 0.7963 - root_mean_squared_error: 0.9513 - mean_squared_logarithmic_error: 0.2048\n",
      "Epoch 20/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.9219 - mse: 0.9219 - mae: 0.8120 - root_mean_squared_error: 0.9601 - mean_squared_logarithmic_error: 0.1981\n",
      "Epoch 21/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.8834 - mse: 0.8834 - mae: 0.7961 - root_mean_squared_error: 0.9399 - mean_squared_logarithmic_error: 0.1989\n",
      "Epoch 22/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.8813 - mse: 0.8813 - mae: 0.7891 - root_mean_squared_error: 0.9388 - mean_squared_logarithmic_error: 0.2080\n",
      "Epoch 23/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.8664 - mse: 0.8664 - mae: 0.7743 - root_mean_squared_error: 0.9308 - mean_squared_logarithmic_error: 0.2006\n",
      "Epoch 24/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.8808 - mse: 0.8808 - mae: 0.7798 - root_mean_squared_error: 0.9385 - mean_squared_logarithmic_error: 0.1893\n",
      "Epoch 25/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.9221 - mse: 0.9221 - mae: 0.8135 - root_mean_squared_error: 0.9603 - mean_squared_logarithmic_error: 0.2047\n",
      "Epoch 26/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.9332 - mse: 0.9332 - mae: 0.7936 - root_mean_squared_error: 0.9660 - mean_squared_logarithmic_error: 0.2179\n",
      "Epoch 27/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.9212 - mse: 0.9212 - mae: 0.8066 - root_mean_squared_error: 0.9598 - mean_squared_logarithmic_error: 0.2120\n",
      "Epoch 28/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.9200 - mse: 0.9200 - mae: 0.8069 - root_mean_squared_error: 0.9591 - mean_squared_logarithmic_error: 0.2038\n",
      "Epoch 29/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.9140 - mse: 0.9140 - mae: 0.8110 - root_mean_squared_error: 0.9560 - mean_squared_logarithmic_error: 0.1986\n",
      "Epoch 30/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.9289 - mse: 0.9289 - mae: 0.8144 - root_mean_squared_error: 0.9638 - mean_squared_logarithmic_error: 0.2041\n",
      "Epoch 31/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.8892 - mse: 0.8892 - mae: 0.7944 - root_mean_squared_error: 0.9430 - mean_squared_logarithmic_error: 0.2014\n",
      "Epoch 32/100\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8456 - mse: 0.8456 - mae: 0.7703 - root_mean_squared_error: 0.9196 - mean_squared_logarithmic_error: 0.1985\n",
      "Epoch 33/100\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8915 - mse: 0.8915 - mae: 0.7960 - root_mean_squared_error: 0.9442 - mean_squared_logarithmic_error: 0.1970\n",
      "Epoch 34/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.9070 - mse: 0.9070 - mae: 0.8124 - root_mean_squared_error: 0.9524 - mean_squared_logarithmic_error: 0.2039\n",
      "Epoch 35/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.8196 - mse: 0.8196 - mae: 0.7635 - root_mean_squared_error: 0.9053 - mean_squared_logarithmic_error: 0.1785\n",
      "Epoch 36/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.8578 - mse: 0.8578 - mae: 0.7778 - root_mean_squared_error: 0.9262 - mean_squared_logarithmic_error: 0.1958\n",
      "Epoch 37/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.8232 - mse: 0.8232 - mae: 0.7549 - root_mean_squared_error: 0.9073 - mean_squared_logarithmic_error: 0.1905\n",
      "Epoch 38/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.9100 - mse: 0.9100 - mae: 0.7796 - root_mean_squared_error: 0.9539 - mean_squared_logarithmic_error: 0.2010\n",
      "Epoch 39/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.8256 - mse: 0.8256 - mae: 0.7505 - root_mean_squared_error: 0.9086 - mean_squared_logarithmic_error: 0.1860\n",
      "Epoch 40/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.7991 - mse: 0.7991 - mae: 0.7479 - root_mean_squared_error: 0.8939 - mean_squared_logarithmic_error: 0.1861\n",
      "Epoch 41/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.8725 - mse: 0.8725 - mae: 0.7541 - root_mean_squared_error: 0.9341 - mean_squared_logarithmic_error: 0.2015\n",
      "Epoch 42/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.8325 - mse: 0.8325 - mae: 0.7482 - root_mean_squared_error: 0.9124 - mean_squared_logarithmic_error: 0.1867\n",
      "Epoch 43/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.8093 - mse: 0.8093 - mae: 0.7361 - root_mean_squared_error: 0.8996 - mean_squared_logarithmic_error: 0.1796\n",
      "Epoch 44/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.8094 - mse: 0.8094 - mae: 0.7505 - root_mean_squared_error: 0.8997 - mean_squared_logarithmic_error: 0.1841\n",
      "Epoch 45/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.7618 - mse: 0.7618 - mae: 0.7160 - root_mean_squared_error: 0.8728 - mean_squared_logarithmic_error: 0.1763\n",
      "Epoch 46/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 5ms/step - loss: 0.7830 - mse: 0.7830 - mae: 0.7287 - root_mean_squared_error: 0.8849 - mean_squared_logarithmic_error: 0.1802\n",
      "Epoch 47/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.8172 - mse: 0.8172 - mae: 0.7543 - root_mean_squared_error: 0.9040 - mean_squared_logarithmic_error: 0.1802\n",
      "Epoch 48/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.8092 - mse: 0.8092 - mae: 0.7532 - root_mean_squared_error: 0.8995 - mean_squared_logarithmic_error: 0.1830\n",
      "Epoch 49/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.8308 - mse: 0.8308 - mae: 0.7419 - root_mean_squared_error: 0.9115 - mean_squared_logarithmic_error: 0.1987\n",
      "Epoch 50/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.7896 - mse: 0.7896 - mae: 0.7364 - root_mean_squared_error: 0.8886 - mean_squared_logarithmic_error: 0.1824\n",
      "Epoch 51/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.8279 - mse: 0.8279 - mae: 0.7774 - root_mean_squared_error: 0.9099 - mean_squared_logarithmic_error: 0.1891\n",
      "Epoch 52/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.7796 - mse: 0.7796 - mae: 0.7429 - root_mean_squared_error: 0.8829 - mean_squared_logarithmic_error: 0.1699\n",
      "Epoch 53/100\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7968 - mse: 0.7968 - mae: 0.7275 - root_mean_squared_error: 0.8926 - mean_squared_logarithmic_error: 0.1844\n",
      "Epoch 54/100\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8376 - mse: 0.8376 - mae: 0.7201 - root_mean_squared_error: 0.9152 - mean_squared_logarithmic_error: 0.1957\n",
      "Epoch 55/100\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8206 - mse: 0.8206 - mae: 0.7270 - root_mean_squared_error: 0.9059 - mean_squared_logarithmic_error: 0.1936\n",
      "Epoch 56/100\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7376 - mse: 0.7376 - mae: 0.7177 - root_mean_squared_error: 0.8588 - mean_squared_logarithmic_error: 0.1673\n",
      "Epoch 57/100\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7390 - mse: 0.7390 - mae: 0.7075 - root_mean_squared_error: 0.8597 - mean_squared_logarithmic_error: 0.1670\n",
      "Epoch 58/100\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7551 - mse: 0.7551 - mae: 0.7105 - root_mean_squared_error: 0.8690 - mean_squared_logarithmic_error: 0.1786\n",
      "Epoch 59/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7549 - mse: 0.7549 - mae: 0.6986 - root_mean_squared_error: 0.8688 - mean_squared_logarithmic_error: 0.1745\n",
      "Epoch 60/100\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.6826 - mse: 0.6826 - mae: 0.6684 - root_mean_squared_error: 0.8262 - mean_squared_logarithmic_error: 0.1497\n",
      "Epoch 61/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.8050 - mse: 0.8050 - mae: 0.7250 - root_mean_squared_error: 0.8972 - mean_squared_logarithmic_error: 0.1782\n",
      "Epoch 62/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.7869 - mse: 0.7869 - mae: 0.7161 - root_mean_squared_error: 0.8871 - mean_squared_logarithmic_error: 0.1758\n",
      "Epoch 63/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.7530 - mse: 0.7530 - mae: 0.6864 - root_mean_squared_error: 0.8677 - mean_squared_logarithmic_error: 0.1700\n",
      "Epoch 64/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.7123 - mse: 0.7123 - mae: 0.6682 - root_mean_squared_error: 0.8440 - mean_squared_logarithmic_error: 0.1616\n",
      "Epoch 65/100\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.6984 - mse: 0.6984 - mae: 0.6644 - root_mean_squared_error: 0.8357 - mean_squared_logarithmic_error: 0.1598\n",
      "Epoch 66/100\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8028 - mse: 0.8028 - mae: 0.7127 - root_mean_squared_error: 0.8960 - mean_squared_logarithmic_error: 0.1878\n",
      "Epoch 67/100\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7855 - mse: 0.7855 - mae: 0.7038 - root_mean_squared_error: 0.8863 - mean_squared_logarithmic_error: 0.1792\n",
      "Epoch 68/100\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7396 - mse: 0.7396 - mae: 0.6866 - root_mean_squared_error: 0.8600 - mean_squared_logarithmic_error: 0.1651\n",
      "Epoch 69/100\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7124 - mse: 0.7124 - mae: 0.6783 - root_mean_squared_error: 0.8440 - mean_squared_logarithmic_error: 0.1626\n",
      "Epoch 70/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.7794 - mse: 0.7794 - mae: 0.6951 - root_mean_squared_error: 0.8828 - mean_squared_logarithmic_error: 0.1800\n",
      "Epoch 71/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.6999 - mse: 0.6999 - mae: 0.6729 - root_mean_squared_error: 0.8366 - mean_squared_logarithmic_error: 0.1664\n",
      "Epoch 72/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.7650 - mse: 0.7650 - mae: 0.7027 - root_mean_squared_error: 0.8746 - mean_squared_logarithmic_error: 0.1796\n",
      "Epoch 73/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.7230 - mse: 0.7230 - mae: 0.6742 - root_mean_squared_error: 0.8503 - mean_squared_logarithmic_error: 0.1646\n",
      "Epoch 74/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.6877 - mse: 0.6877 - mae: 0.6617 - root_mean_squared_error: 0.8293 - mean_squared_logarithmic_error: 0.1576\n",
      "Epoch 75/100\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7051 - mse: 0.7051 - mae: 0.6628 - root_mean_squared_error: 0.8397 - mean_squared_logarithmic_error: 0.1701\n",
      "Epoch 76/100\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7153 - mse: 0.7153 - mae: 0.6795 - root_mean_squared_error: 0.8457 - mean_squared_logarithmic_error: 0.1645\n",
      "Epoch 77/100\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7525 - mse: 0.7525 - mae: 0.6928 - root_mean_squared_error: 0.8675 - mean_squared_logarithmic_error: 0.1709\n",
      "Epoch 78/100\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7342 - mse: 0.7342 - mae: 0.6748 - root_mean_squared_error: 0.8568 - mean_squared_logarithmic_error: 0.1701\n",
      "Epoch 79/100\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.6993 - mse: 0.6993 - mae: 0.6700 - root_mean_squared_error: 0.8363 - mean_squared_logarithmic_error: 0.1642\n",
      "Epoch 80/100\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7262 - mse: 0.7262 - mae: 0.6709 - root_mean_squared_error: 0.8521 - mean_squared_logarithmic_error: 0.1719\n",
      "Epoch 81/100\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7487 - mse: 0.7487 - mae: 0.6766 - root_mean_squared_error: 0.8653 - mean_squared_logarithmic_error: 0.1723\n",
      "Epoch 82/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.7140 - mse: 0.7140 - mae: 0.6826 - root_mean_squared_error: 0.8450 - mean_squared_logarithmic_error: 0.1709\n",
      "Epoch 83/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.7135 - mse: 0.7135 - mae: 0.6631 - root_mean_squared_error: 0.8447 - mean_squared_logarithmic_error: 0.1680\n",
      "Epoch 84/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.7189 - mse: 0.7189 - mae: 0.6697 - root_mean_squared_error: 0.8479 - mean_squared_logarithmic_error: 0.1577\n",
      "Epoch 85/100\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.6792 - mse: 0.6792 - mae: 0.6527 - root_mean_squared_error: 0.8241 - mean_squared_logarithmic_error: 0.1632\n",
      "Epoch 86/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.7243 - mse: 0.7243 - mae: 0.6772 - root_mean_squared_error: 0.8511 - mean_squared_logarithmic_error: 0.1715\n",
      "Epoch 87/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.7000 - mse: 0.7000 - mae: 0.6549 - root_mean_squared_error: 0.8367 - mean_squared_logarithmic_error: 0.1603\n",
      "Epoch 88/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.6721 - mse: 0.6721 - mae: 0.6485 - root_mean_squared_error: 0.8198 - mean_squared_logarithmic_error: 0.1596\n",
      "Epoch 89/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.7149 - mse: 0.7149 - mae: 0.6800 - root_mean_squared_error: 0.8455 - mean_squared_logarithmic_error: 0.1628\n",
      "Epoch 90/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.7226 - mse: 0.7226 - mae: 0.6926 - root_mean_squared_error: 0.8501 - mean_squared_logarithmic_error: 0.1608\n",
      "Epoch 91/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.6414 - mse: 0.6414 - mae: 0.6280 - root_mean_squared_error: 0.8009 - mean_squared_logarithmic_error: 0.1531\n",
      "Epoch 92/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.7065 - mse: 0.7065 - mae: 0.6588 - root_mean_squared_error: 0.8406 - mean_squared_logarithmic_error: 0.1627\n",
      "Epoch 93/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.8079 - mse: 0.8079 - mae: 0.7121 - root_mean_squared_error: 0.8988 - mean_squared_logarithmic_error: 0.1934\n",
      "Epoch 94/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.6815 - mse: 0.6815 - mae: 0.6509 - root_mean_squared_error: 0.8255 - mean_squared_logarithmic_error: 0.1646\n",
      "Epoch 95/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.7327 - mse: 0.7327 - mae: 0.6602 - root_mean_squared_error: 0.8560 - mean_squared_logarithmic_error: 0.1669\n",
      "Epoch 96/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.6570 - mse: 0.6570 - mae: 0.6271 - root_mean_squared_error: 0.8105 - mean_squared_logarithmic_error: 0.1602\n",
      "Epoch 97/100\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.6609 - mse: 0.6609 - mae: 0.6347 - root_mean_squared_error: 0.8130 - mean_squared_logarithmic_error: 0.1565\n",
      "Epoch 98/100\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.6761 - mse: 0.6761 - mae: 0.6465 - root_mean_squared_error: 0.8223 - mean_squared_logarithmic_error: 0.1542\n",
      "Epoch 99/100\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7418 - mse: 0.7418 - mae: 0.6737 - root_mean_squared_error: 0.8613 - mean_squared_logarithmic_error: 0.1647\n",
      "Epoch 100/100\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.6928 - mse: 0.6928 - mae: 0.6425 - root_mean_squared_error: 0.8323 - mean_squared_logarithmic_error: 0.1610\n"
     ]
    }
   ],
   "source": [
    "# Fitting the model on training data.\n",
    "import time\n",
    "t0=time.time()\n",
    "history = model.fit(train_x, train_y, epochs = 100, batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"training time:\", round(time.time()-t0, 3), \"s\") # the time would be round to 3 decimal in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.4400729 ],\n",
       "       [2.4712381 ],\n",
       "       [0.16822293],\n",
       "       [1.7029064 ],\n",
       "       [2.04747   ],\n",
       "       [1.1755701 ],\n",
       "       [1.5980489 ],\n",
       "       [0.        ],\n",
       "       [0.20402032],\n",
       "       [1.4777297 ],\n",
       "       [1.5900168 ],\n",
       "       [2.1438358 ],\n",
       "       [0.20186529],\n",
       "       [1.4266883 ],\n",
       "       [2.0835066 ],\n",
       "       [0.        ],\n",
       "       [1.0606157 ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.49948457],\n",
       "       [0.58904487],\n",
       "       [0.08472293],\n",
       "       [0.36580038],\n",
       "       [2.0389624 ],\n",
       "       [2.2882586 ],\n",
       "       [1.8173517 ],\n",
       "       [1.0344427 ],\n",
       "       [0.45675364],\n",
       "       [0.9915461 ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [1.4686266 ],\n",
       "       [0.27127826],\n",
       "       [0.        ],\n",
       "       [1.6224047 ],\n",
       "       [0.32644144],\n",
       "       [0.48913845],\n",
       "       [0.24816355],\n",
       "       [1.8140963 ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.85075223],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.97794116],\n",
       "       [1.8891755 ],\n",
       "       [2.5818927 ],\n",
       "       [0.07405045],\n",
       "       [1.3446535 ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [1.8491138 ],\n",
       "       [1.6500934 ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.4122193 ],\n",
       "       [0.93901664],\n",
       "       [1.9319898 ],\n",
       "       [0.        ],\n",
       "       [2.4544513 ],\n",
       "       [0.33798227],\n",
       "       [1.7491136 ],\n",
       "       [2.12122   ],\n",
       "       [0.93988913],\n",
       "       [0.        ],\n",
       "       [2.0895205 ],\n",
       "       [0.        ],\n",
       "       [1.7481788 ],\n",
       "       [1.0466919 ],\n",
       "       [1.9992021 ],\n",
       "       [2.4294763 ],\n",
       "       [2.0375366 ],\n",
       "       [0.        ],\n",
       "       [1.747068  ],\n",
       "       [1.0544425 ],\n",
       "       [2.6575336 ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [1.3404815 ],\n",
       "       [1.022375  ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.3129247 ],\n",
       "       [1.7419739 ],\n",
       "       [1.8838948 ],\n",
       "       [1.7279887 ],\n",
       "       [0.76421386],\n",
       "       [1.5687652 ],\n",
       "       [0.2459119 ],\n",
       "       [1.9727274 ],\n",
       "       [1.1426467 ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [1.9803177 ],\n",
       "       [0.        ],\n",
       "       [1.2996744 ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.80633515],\n",
       "       [0.11719381],\n",
       "       [1.7193061 ],\n",
       "       [0.02365749],\n",
       "       [0.        ],\n",
       "       [0.46362525],\n",
       "       [2.3879893 ],\n",
       "       [0.26493365],\n",
       "       [1.7617799 ],\n",
       "       [0.        ],\n",
       "       [0.4186257 ],\n",
       "       [0.9640989 ],\n",
       "       [0.56456345],\n",
       "       [1.623938  ],\n",
       "       [1.866067  ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [1.7563063 ],\n",
       "       [1.874287  ],\n",
       "       [0.        ],\n",
       "       [0.63219076],\n",
       "       [0.3772176 ],\n",
       "       [0.28905812],\n",
       "       [1.2059392 ],\n",
       "       [1.7631125 ],\n",
       "       [1.171394  ],\n",
       "       [1.2132951 ],\n",
       "       [0.        ],\n",
       "       [1.3688534 ],\n",
       "       [1.0593157 ],\n",
       "       [0.84522396],\n",
       "       [0.        ],\n",
       "       [0.7443295 ],\n",
       "       [1.174969  ],\n",
       "       [1.9448322 ],\n",
       "       [0.        ],\n",
       "       [0.9635411 ],\n",
       "       [0.51970845],\n",
       "       [0.        ],\n",
       "       [1.7182394 ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [1.244279  ],\n",
       "       [1.5543693 ],\n",
       "       [1.333651  ],\n",
       "       [0.76305705],\n",
       "       [0.32860473],\n",
       "       [1.7500122 ],\n",
       "       [0.        ],\n",
       "       [0.83421516],\n",
       "       [1.5344255 ],\n",
       "       [0.        ],\n",
       "       [1.9184114 ],\n",
       "       [1.3302754 ],\n",
       "       [2.255401  ],\n",
       "       [1.7290891 ],\n",
       "       [0.03667738],\n",
       "       [0.8989658 ],\n",
       "       [2.0354369 ],\n",
       "       [1.4854211 ],\n",
       "       [1.6310229 ],\n",
       "       [1.3808768 ],\n",
       "       [1.5091583 ],\n",
       "       [0.        ],\n",
       "       [0.58974415],\n",
       "       [0.        ],\n",
       "       [1.3643187 ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [1.6644062 ],\n",
       "       [1.5129509 ],\n",
       "       [1.0429202 ],\n",
       "       [0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the predictions.\n",
    "t1=time.time()\n",
    "predictions_y = model.predict(test_x)\n",
    "print(\"predict time:\", round(time.time()-t1, 3), \"s\")\n",
    "predictions_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [2.],\n",
       "       [0.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [2.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [2.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [3.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [0.],\n",
       "       [2.],\n",
       "       [0.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [2.],\n",
       "       [0.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [0.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [3.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [0.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [2.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [2.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [2.],\n",
       "       [0.],\n",
       "       [2.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [2.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [2.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [0.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [0.]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rounding off the predictions to nearest\n",
    "#integer as count of bugs is an integer.\n",
    "predictions_y_round = np.rint(predictions_y)\n",
    "predictions_y_round "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Viewing the number of predictions.\n",
    "predictions_y_round.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([156.], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the sum of all the predictions obtained to used while obtaining FPA\n",
    "s = 0\n",
    "for  t in range(predictions_y_round.shape[0]):\n",
    "    s+=predictions_y_round[t]\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5051857], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtaining the value of FPA metric for the model\n",
    "Fpa = 0\n",
    "for  t in range(predictions_y_round.shape[0]):\n",
    "        x = 0\n",
    "        for j in range( predictions_y_round.shape[0]-t+1, predictions_y_round.shape[0]):\n",
    "               x = x + predictions_y_round[j]\n",
    "        \n",
    "        x = (x/s)/predictions_y_round.shape[0]\n",
    "        Fpa = Fpa + x\n",
    "Fpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.513577], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtaining the value of CLC metric for the model\n",
    "previous_obtained = predictions_y_round[predictions_y_round.shape[0] - 1]/s\n",
    "\n",
    "CLC = 0\n",
    "for i in range(predictions_y_round.shape[0]):\n",
    "    if(i==0):\n",
    "        CLC += 0 + previous_obtained\n",
    "    else:\n",
    "        additional = (predictions_y_round[predictions_y_round.shape[0] - 1 - i])/s\n",
    "        CLC += 2*previous_obtained + additional\n",
    "        previous_obtained += additional\n",
    "        \n",
    "CLC/=(2*predictions_y_round.shape[0])\n",
    "CLC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 3ms/step - loss: 1.2263 - mse: 1.2263 - mae: 0.8271 - root_mean_squared_error: 1.1074 - mean_squared_logarithmic_error: 0.4133\n",
      "dict_keys(['loss', 'mse', 'mae', 'root_mean_squared_error', 'mean_squared_logarithmic_error'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['loss',\n",
       " 'mse',\n",
       " 'mae',\n",
       " 'root_mean_squared_error',\n",
       " 'mean_squared_logarithmic_error']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting direct metric results using the metrics given to model.\n",
    "score = model.evaluate(test_x, test_y)\n",
    "print(history.history.keys())\n",
    "model.test_on_batch(test_x, test_y)\n",
    "model.metrics_names\n",
    "#print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\n",
    "#print(\"\\n%s: %.2f%%\" % (model.metrics_names[2], score[2]*100))\n",
    "#print(\"\\n%s: %.2f%%\" % (model.metrics_names[3], score[3]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/bavanya/Downloads/deb_packages/home/bavanya/Desktop/venv_python/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /home/bavanya/Downloads/deb_packages/home/bavanya/Desktop/venv_python/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: /home/bavanya/Desktop/6thSem/BTP/regression_PROMISE_dataset/saved_models/ant1.3_ant1.4_model4/ant1.3_ant1.4_model4_3/assets\n"
     ]
    }
   ],
   "source": [
    "# Saving the model\n",
    "model_id = 11\n",
    "path_to_save = '/home/bavanya/Desktop/6thSem/BTP/regression_PROMISE_dataset/saved_models/ant1.3_ant1.4_model4/ant1.3_ant1.4_model4_3'\n",
    "model.save(path_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the results to csv file.\n",
    "heading = ['model_id', 'train_data_name', 'test_data_name'] + model.metrics_names + ['fpa', 'clc']\n",
    "score = [model_id, train_data_name, test_data_name] + score + [float(Fpa) , float(CLC)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_id',\n",
       " 'train_data_name',\n",
       " 'test_data_name',\n",
       " 'loss',\n",
       " 'mse',\n",
       " 'mae',\n",
       " 'root_mean_squared_error',\n",
       " 'mean_squared_logarithmic_error',\n",
       " 'fpa',\n",
       " 'clc']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11,\n",
       " 'ant-1.3',\n",
       " 'ant-1.4',\n",
       " 1.226317048072815,\n",
       " 1.226317048072815,\n",
       " 0.8270690441131592,\n",
       " 1.1073919534683228,\n",
       " 0.41329681873321533,\n",
       " 0.5051857233047485,\n",
       " 0.5135769844055176]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the results to csv file.\n",
    "with open(path_to_save + '_metric_results.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(heading)\n",
    "    writer.writerow(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model and to make sure that the model is saved properly.\n",
    "model_loaded = load_model(path_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
