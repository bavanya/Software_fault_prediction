{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MID 7\n",
    "### Taking ant1.3 as training data.\n",
    "### ant1.4 as testing data.\n",
    "### Min max scaling done to few columns: ['wmc', 'dit', 'noc', 'cbo', 'rfc', 'lcom', 'ca', 'ce', 'npm', 'lcom3', 'loc', 'dam', 'moa', 'mfa', 'cam', 'ic', 'cbm', 'amc', 'max_cc', 'avg_cc']\n",
    "### SVD used for feature reduction, n_components = 15.\n",
    "### Oversampling and smote methods used to increase size of training data.\n",
    "###  Deep neural network used, model type is 3 as per the BTP documentation spreadsheet.\n",
    "### np.rint() used on predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time \n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dense\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data. \n",
    "train_data_path = \"/home/bavanya/Desktop/6thSem/BTP/regression_PROMISE_dataset/datasets/ant-1.3.csv\"\n",
    "test_data_path = \"/home/bavanya/Desktop/6thSem/BTP/regression_PROMISE_dataset/datasets/ant-1.4.csv\"\n",
    "train_data_name = \"ant-1.3\"\n",
    "test_data_name = \"ant-1.4\"\n",
    "ant_1_3 = pd.read_csv(train_data_path)\n",
    "ant_1_4 = pd.read_csv(test_data_path)\n",
    "files = [\"/home/bavanya/Desktop/6thSem/BTP/regression_PROMISE_dataset/datasets/ant-1.3.csv\", \"/home/bavanya/Desktop/6thSem/BTP/regression_PROMISE_dataset/datasets/ant-1.4.csv\"]\n",
    "combined_data = pd.concat(map(pd.read_csv, files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Min Max Scaling.\n",
    "scaler = MinMaxScaler()\n",
    "MinMaxScaler(copy=True, feature_range=(0, 1))\n",
    "cols_to_norm = ['wmc', 'dit', 'noc', 'cbo', 'rfc', 'lcom', 'ca', 'ce', 'npm', 'lcom3', 'loc', 'dam', 'moa', 'mfa', 'cam', 'ic', 'cbm', 'amc', 'max_cc', 'avg_cc']\n",
    "combined_data[cols_to_norm] = MinMaxScaler().fit_transform(combined_data[cols_to_norm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>version</th>\n",
       "      <th>name.1</th>\n",
       "      <th>wmc</th>\n",
       "      <th>dit</th>\n",
       "      <th>noc</th>\n",
       "      <th>cbo</th>\n",
       "      <th>rfc</th>\n",
       "      <th>lcom</th>\n",
       "      <th>ca</th>\n",
       "      <th>...</th>\n",
       "      <th>dam</th>\n",
       "      <th>moa</th>\n",
       "      <th>mfa</th>\n",
       "      <th>cam</th>\n",
       "      <th>ic</th>\n",
       "      <th>cbm</th>\n",
       "      <th>amc</th>\n",
       "      <th>max_cc</th>\n",
       "      <th>avg_cc</th>\n",
       "      <th>bug</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ant</td>\n",
       "      <td>1.3</td>\n",
       "      <td>org.apache.tools.ant.taskdefs.ExecuteOn</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.102941</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.011856</td>\n",
       "      <td>0.014815</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.885057</td>\n",
       "      <td>0.232323</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.165951</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>0.209085</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ant</td>\n",
       "      <td>1.3</td>\n",
       "      <td>org.apache.tools.ant.DefaultLogger</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.163265</td>\n",
       "      <td>0.020033</td>\n",
       "      <td>0.029630</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080979</td>\n",
       "      <td>0.171429</td>\n",
       "      <td>0.269903</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ant</td>\n",
       "      <td>1.3</td>\n",
       "      <td>org.apache.tools.ant.taskdefs.TaskOutputStream</td>\n",
       "      <td>0.038961</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.007353</td>\n",
       "      <td>0.045918</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.083267</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.109529</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ant</td>\n",
       "      <td>1.3</td>\n",
       "      <td>org.apache.tools.ant.taskdefs.Cvs</td>\n",
       "      <td>0.155844</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.088235</td>\n",
       "      <td>0.188776</td>\n",
       "      <td>0.013083</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115693</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>0.232742</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ant</td>\n",
       "      <td>1.3</td>\n",
       "      <td>org.apache.tools.ant.taskdefs.Copyfile</td>\n",
       "      <td>0.077922</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.880952</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.100881</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.136898</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>ant</td>\n",
       "      <td>1.4</td>\n",
       "      <td>org.apache.tools.ant.TaskAdapter</td>\n",
       "      <td>0.064935</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.051471</td>\n",
       "      <td>0.096939</td>\n",
       "      <td>0.001635</td>\n",
       "      <td>0.029630</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.138351</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.131428</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>ant</td>\n",
       "      <td>1.4</td>\n",
       "      <td>org.apache.tools.ant.taskdefs.rmic.DefaultRmic...</td>\n",
       "      <td>0.220779</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.110294</td>\n",
       "      <td>0.336735</td>\n",
       "      <td>0.036795</td>\n",
       "      <td>0.029630</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.197917</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.172373</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.396221</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>ant</td>\n",
       "      <td>1.4</td>\n",
       "      <td>org.apache.tools.ant.IntrospectionHelper</td>\n",
       "      <td>0.298701</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.227941</td>\n",
       "      <td>0.418367</td>\n",
       "      <td>0.082175</td>\n",
       "      <td>0.162963</td>\n",
       "      <td>...</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.219724</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.378561</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>ant</td>\n",
       "      <td>1.4</td>\n",
       "      <td>org.apache.tools.ant.taskdefs.compilers.Defaul...</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.161765</td>\n",
       "      <td>0.438776</td>\n",
       "      <td>0.006132</td>\n",
       "      <td>0.059259</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.373390</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.761689</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>ant</td>\n",
       "      <td>1.4</td>\n",
       "      <td>org.apache.tools.ant.NoBannerLogger</td>\n",
       "      <td>0.051948</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.022059</td>\n",
       "      <td>0.081633</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.064852</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.287498</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>303 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    name  version                                             name.1  \\\n",
       "0    ant      1.3            org.apache.tools.ant.taskdefs.ExecuteOn   \n",
       "1    ant      1.3                 org.apache.tools.ant.DefaultLogger   \n",
       "2    ant      1.3     org.apache.tools.ant.taskdefs.TaskOutputStream   \n",
       "3    ant      1.3                  org.apache.tools.ant.taskdefs.Cvs   \n",
       "4    ant      1.3             org.apache.tools.ant.taskdefs.Copyfile   \n",
       "..   ...      ...                                                ...   \n",
       "173  ant      1.4                   org.apache.tools.ant.TaskAdapter   \n",
       "174  ant      1.4  org.apache.tools.ant.taskdefs.rmic.DefaultRmic...   \n",
       "175  ant      1.4           org.apache.tools.ant.IntrospectionHelper   \n",
       "176  ant      1.4  org.apache.tools.ant.taskdefs.compilers.Defaul...   \n",
       "177  ant      1.4                org.apache.tools.ant.NoBannerLogger   \n",
       "\n",
       "          wmc  dit    noc       cbo       rfc      lcom        ca  ...  \\\n",
       "0    0.142857  0.6  0.050  0.102941  0.214286  0.011856  0.014815  ...   \n",
       "1    0.181818  0.0  0.025  0.058824  0.163265  0.020033  0.029630  ...   \n",
       "2    0.038961  0.2  0.000  0.007353  0.045918  0.000000  0.000000  ...   \n",
       "3    0.155844  0.4  0.000  0.088235  0.188776  0.013083  0.000000  ...   \n",
       "4    0.077922  0.4  0.000  0.029412  0.107143  0.000409  0.000000  ...   \n",
       "..        ...  ...    ...       ...       ...       ...       ...  ...   \n",
       "173  0.064935  0.4  0.000  0.051471  0.096939  0.001635  0.029630  ...   \n",
       "174  0.220779  0.0  0.075  0.110294  0.336735  0.036795  0.029630  ...   \n",
       "175  0.298701  0.0  0.000  0.227941  0.418367  0.082175  0.162963  ...   \n",
       "176  0.142857  0.0  0.200  0.161765  0.438776  0.006132  0.059259  ...   \n",
       "177  0.051948  0.2  0.000  0.022059  0.081633  0.000000  0.000000  ...   \n",
       "\n",
       "          dam       moa       mfa       cam    ic       cbm       amc  \\\n",
       "0    1.000000  0.111111  0.885057  0.232323  0.75  0.363636  0.165951   \n",
       "1    1.000000  0.000000  0.000000  0.307692  0.00  0.000000  0.080979   \n",
       "2    1.000000  0.111111  0.714286  0.666667  0.25  0.090909  0.083267   \n",
       "3    1.000000  0.111111  0.770833  0.458333  0.00  0.000000  0.115693   \n",
       "4    1.000000  0.000000  0.880952  0.416667  0.50  0.181818  0.100881   \n",
       "..        ...       ...       ...       ...   ...       ...       ...   \n",
       "173  0.000000  0.000000  0.902439  0.400000  0.25  0.090909  0.138351   \n",
       "174  1.000000  0.222222  0.000000  0.197917  0.00  0.000000  0.172373   \n",
       "175  0.444444  0.000000  0.000000  0.318182  0.00  0.000000  0.219724   \n",
       "176  1.000000  0.777778  0.000000  0.266667  0.00  0.000000  0.373390   \n",
       "177  1.000000  0.000000  0.842105  0.875000  0.25  0.090909  0.064852   \n",
       "\n",
       "       max_cc    avg_cc  bug  \n",
       "0    0.085714  0.209085    0  \n",
       "1    0.171429  0.269903    2  \n",
       "2    0.028571  0.109529    0  \n",
       "3    0.085714  0.232742    0  \n",
       "4    0.028571  0.136898    0  \n",
       "..        ...       ...  ...  \n",
       "173  0.028571  0.131428    0  \n",
       "174  0.400000  0.396221    0  \n",
       "175  0.685714  0.378561    0  \n",
       "176  0.628571  0.761689    1  \n",
       "177  0.142857  0.287498    0  \n",
       "\n",
       "[303 rows x 24 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define transform\n",
    "components = 15\n",
    "svd = TruncatedSVD(n_components=components)\n",
    "# prepare transform on dataset\n",
    "svd.fit(combined_data[cols_to_norm])\n",
    "# apply transform to dataset\n",
    "transformed = svd.transform(combined_data[cols_to_norm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.697270</td>\n",
       "      <td>-0.240716</td>\n",
       "      <td>0.571264</td>\n",
       "      <td>0.167873</td>\n",
       "      <td>-0.202270</td>\n",
       "      <td>-0.188764</td>\n",
       "      <td>-0.085932</td>\n",
       "      <td>0.036019</td>\n",
       "      <td>-0.075786</td>\n",
       "      <td>0.129869</td>\n",
       "      <td>-0.074938</td>\n",
       "      <td>-0.025339</td>\n",
       "      <td>-0.066285</td>\n",
       "      <td>0.001433</td>\n",
       "      <td>-0.015661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.016611</td>\n",
       "      <td>-0.267720</td>\n",
       "      <td>-0.587890</td>\n",
       "      <td>-0.054598</td>\n",
       "      <td>-0.014350</td>\n",
       "      <td>-0.077750</td>\n",
       "      <td>-0.160155</td>\n",
       "      <td>0.052980</td>\n",
       "      <td>0.100634</td>\n",
       "      <td>-0.030916</td>\n",
       "      <td>-0.066994</td>\n",
       "      <td>-0.048365</td>\n",
       "      <td>-0.007133</td>\n",
       "      <td>0.003084</td>\n",
       "      <td>0.027329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.298451</td>\n",
       "      <td>-0.173527</td>\n",
       "      <td>0.139837</td>\n",
       "      <td>-0.541003</td>\n",
       "      <td>-0.007964</td>\n",
       "      <td>0.011583</td>\n",
       "      <td>0.156168</td>\n",
       "      <td>-0.110641</td>\n",
       "      <td>-0.028186</td>\n",
       "      <td>0.008754</td>\n",
       "      <td>0.139273</td>\n",
       "      <td>-0.107446</td>\n",
       "      <td>-0.018334</td>\n",
       "      <td>0.006414</td>\n",
       "      <td>-0.066103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.537891</td>\n",
       "      <td>-0.056428</td>\n",
       "      <td>0.028502</td>\n",
       "      <td>-0.104266</td>\n",
       "      <td>0.280907</td>\n",
       "      <td>0.130204</td>\n",
       "      <td>-0.176106</td>\n",
       "      <td>-0.029262</td>\n",
       "      <td>-0.088801</td>\n",
       "      <td>-0.025663</td>\n",
       "      <td>-0.118818</td>\n",
       "      <td>0.003933</td>\n",
       "      <td>0.018829</td>\n",
       "      <td>-0.043154</td>\n",
       "      <td>0.048522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.530257</td>\n",
       "      <td>-0.074334</td>\n",
       "      <td>0.374525</td>\n",
       "      <td>-0.205480</td>\n",
       "      <td>-0.061914</td>\n",
       "      <td>-0.152907</td>\n",
       "      <td>-0.063597</td>\n",
       "      <td>0.021766</td>\n",
       "      <td>0.089365</td>\n",
       "      <td>0.037752</td>\n",
       "      <td>0.031946</td>\n",
       "      <td>-0.114260</td>\n",
       "      <td>-0.091288</td>\n",
       "      <td>-0.014637</td>\n",
       "      <td>-0.040312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>0.886501</td>\n",
       "      <td>0.524176</td>\n",
       "      <td>0.551332</td>\n",
       "      <td>0.103135</td>\n",
       "      <td>0.083555</td>\n",
       "      <td>0.172359</td>\n",
       "      <td>0.095620</td>\n",
       "      <td>0.012480</td>\n",
       "      <td>0.004141</td>\n",
       "      <td>0.017085</td>\n",
       "      <td>0.037964</td>\n",
       "      <td>-0.029034</td>\n",
       "      <td>-0.045604</td>\n",
       "      <td>-0.017821</td>\n",
       "      <td>0.022297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>1.128822</td>\n",
       "      <td>-0.426056</td>\n",
       "      <td>-0.629426</td>\n",
       "      <td>0.258850</td>\n",
       "      <td>-0.034627</td>\n",
       "      <td>0.177394</td>\n",
       "      <td>-0.220035</td>\n",
       "      <td>0.025682</td>\n",
       "      <td>-0.128906</td>\n",
       "      <td>0.061409</td>\n",
       "      <td>-0.099610</td>\n",
       "      <td>-0.022340</td>\n",
       "      <td>0.050520</td>\n",
       "      <td>0.067642</td>\n",
       "      <td>-0.067599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>1.027511</td>\n",
       "      <td>-0.126484</td>\n",
       "      <td>-0.591917</td>\n",
       "      <td>0.769665</td>\n",
       "      <td>0.027848</td>\n",
       "      <td>0.483424</td>\n",
       "      <td>0.049615</td>\n",
       "      <td>-0.134999</td>\n",
       "      <td>-0.142810</td>\n",
       "      <td>0.233455</td>\n",
       "      <td>-0.574456</td>\n",
       "      <td>-0.054281</td>\n",
       "      <td>0.160251</td>\n",
       "      <td>0.095574</td>\n",
       "      <td>-0.128321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>1.318002</td>\n",
       "      <td>-0.514244</td>\n",
       "      <td>-0.775534</td>\n",
       "      <td>0.478137</td>\n",
       "      <td>-0.191742</td>\n",
       "      <td>0.531927</td>\n",
       "      <td>-0.176518</td>\n",
       "      <td>0.103397</td>\n",
       "      <td>-0.589727</td>\n",
       "      <td>0.040915</td>\n",
       "      <td>0.212815</td>\n",
       "      <td>0.047857</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>0.095875</td>\n",
       "      <td>-0.128328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>1.534499</td>\n",
       "      <td>0.032362</td>\n",
       "      <td>0.068811</td>\n",
       "      <td>-0.514331</td>\n",
       "      <td>-0.062047</td>\n",
       "      <td>0.142505</td>\n",
       "      <td>0.205071</td>\n",
       "      <td>-0.085574</td>\n",
       "      <td>0.014607</td>\n",
       "      <td>-0.033795</td>\n",
       "      <td>0.029711</td>\n",
       "      <td>-0.200737</td>\n",
       "      <td>-0.019112</td>\n",
       "      <td>0.003207</td>\n",
       "      <td>-0.035288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>303 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0    1.697270 -0.240716  0.571264  0.167873 -0.202270 -0.188764 -0.085932   \n",
       "1    1.016611 -0.267720 -0.587890 -0.054598 -0.014350 -0.077750 -0.160155   \n",
       "2    1.298451 -0.173527  0.139837 -0.541003 -0.007964  0.011583  0.156168   \n",
       "3    1.537891 -0.056428  0.028502 -0.104266  0.280907  0.130204 -0.176106   \n",
       "4    1.530257 -0.074334  0.374525 -0.205480 -0.061914 -0.152907 -0.063597   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "298  0.886501  0.524176  0.551332  0.103135  0.083555  0.172359  0.095620   \n",
       "299  1.128822 -0.426056 -0.629426  0.258850 -0.034627  0.177394 -0.220035   \n",
       "300  1.027511 -0.126484 -0.591917  0.769665  0.027848  0.483424  0.049615   \n",
       "301  1.318002 -0.514244 -0.775534  0.478137 -0.191742  0.531927 -0.176518   \n",
       "302  1.534499  0.032362  0.068811 -0.514331 -0.062047  0.142505  0.205071   \n",
       "\n",
       "            7         8         9        10        11        12        13  \\\n",
       "0    0.036019 -0.075786  0.129869 -0.074938 -0.025339 -0.066285  0.001433   \n",
       "1    0.052980  0.100634 -0.030916 -0.066994 -0.048365 -0.007133  0.003084   \n",
       "2   -0.110641 -0.028186  0.008754  0.139273 -0.107446 -0.018334  0.006414   \n",
       "3   -0.029262 -0.088801 -0.025663 -0.118818  0.003933  0.018829 -0.043154   \n",
       "4    0.021766  0.089365  0.037752  0.031946 -0.114260 -0.091288 -0.014637   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "298  0.012480  0.004141  0.017085  0.037964 -0.029034 -0.045604 -0.017821   \n",
       "299  0.025682 -0.128906  0.061409 -0.099610 -0.022340  0.050520  0.067642   \n",
       "300 -0.134999 -0.142810  0.233455 -0.574456 -0.054281  0.160251  0.095574   \n",
       "301  0.103397 -0.589727  0.040915  0.212815  0.047857  0.022727  0.095875   \n",
       "302 -0.085574  0.014607 -0.033795  0.029711 -0.200737 -0.019112  0.003207   \n",
       "\n",
       "           14  \n",
       "0   -0.015661  \n",
       "1    0.027329  \n",
       "2   -0.066103  \n",
       "3    0.048522  \n",
       "4   -0.040312  \n",
       "..        ...  \n",
       "298  0.022297  \n",
       "299 -0.067599  \n",
       "300 -0.128321  \n",
       "301 -0.128328  \n",
       "302 -0.035288  \n",
       "\n",
       "[303 rows x 15 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed = pd.DataFrame(transformed)\n",
    "transformed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_index_list = list(range(ant_1_3.shape[0]))\n",
    "train_data_index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 200,\n",
       " 201,\n",
       " 202,\n",
       " 203,\n",
       " 204,\n",
       " 205,\n",
       " 206,\n",
       " 207,\n",
       " 208,\n",
       " 209,\n",
       " 210,\n",
       " 211,\n",
       " 212,\n",
       " 213,\n",
       " 214,\n",
       " 215,\n",
       " 216,\n",
       " 217,\n",
       " 218,\n",
       " 219,\n",
       " 220,\n",
       " 221,\n",
       " 222,\n",
       " 223,\n",
       " 224,\n",
       " 225,\n",
       " 226,\n",
       " 227,\n",
       " 228,\n",
       " 229,\n",
       " 230,\n",
       " 231,\n",
       " 232,\n",
       " 233,\n",
       " 234,\n",
       " 235,\n",
       " 236,\n",
       " 237,\n",
       " 238,\n",
       " 239,\n",
       " 240,\n",
       " 241,\n",
       " 242,\n",
       " 243,\n",
       " 244,\n",
       " 245,\n",
       " 246,\n",
       " 247,\n",
       " 248,\n",
       " 249,\n",
       " 250,\n",
       " 251,\n",
       " 252,\n",
       " 253,\n",
       " 254,\n",
       " 255,\n",
       " 256,\n",
       " 257,\n",
       " 258,\n",
       " 259,\n",
       " 260,\n",
       " 261,\n",
       " 262,\n",
       " 263,\n",
       " 264,\n",
       " 265,\n",
       " 266,\n",
       " 267,\n",
       " 268,\n",
       " 269,\n",
       " 270,\n",
       " 271,\n",
       " 272,\n",
       " 273,\n",
       " 274,\n",
       " 275,\n",
       " 276,\n",
       " 277,\n",
       " 278,\n",
       " 279,\n",
       " 280,\n",
       " 281,\n",
       " 282,\n",
       " 283,\n",
       " 284,\n",
       " 285,\n",
       " 286,\n",
       " 287,\n",
       " 288,\n",
       " 289,\n",
       " 290,\n",
       " 291,\n",
       " 292,\n",
       " 293,\n",
       " 294,\n",
       " 295,\n",
       " 296,\n",
       " 297,\n",
       " 298,\n",
       " 299,\n",
       " 300,\n",
       " 301,\n",
       " 302]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_index_list = list(range(ant_1_3.shape[0], ant_1_3.shape[0] + ant_1_4.shape[0]))\n",
    "test_data_index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = transformed[transformed.index.isin(train_data_index_list)]\n",
    "X_train = np.array(X_train)\n",
    "\n",
    "X_test = transformed[transformed.index.isin(test_data_index_list)]\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = combined_data[transformed.index.isin(train_data_index_list)]\n",
    "Y_train = Y_train['bug']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = combined_data[transformed.index.isin(test_data_index_list)]\n",
    "Y_test = Y_test['bug']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying over sampling and SMOTE to training data for augmentation.\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "X_train, Y_train = ros.fit_resample(X_train, Y_train)\n",
    "\n",
    "smt = SMOTE()\n",
    "X_train, Y_train = smt.fit_resample(X_train, Y_train)\n",
    "\n",
    "train_x = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "test_x = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.69726979e+00, -2.40716233e-01,  5.71263952e-01, ...,\n",
       "         -6.62849517e-02,  1.43327120e-03, -1.56612043e-02]],\n",
       "\n",
       "       [[ 1.01661138e+00, -2.67720025e-01, -5.87889971e-01, ...,\n",
       "         -7.13327593e-03,  3.08443645e-03,  2.73289410e-02]],\n",
       "\n",
       "       [[ 1.29845060e+00, -1.73526753e-01,  1.39837075e-01, ...,\n",
       "         -1.83338282e-02,  6.41379625e-03, -6.61028019e-02]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 1.01420045e+00, -3.74213023e-01, -5.47215559e-01, ...,\n",
       "          3.08422923e-02, -4.45360308e-02, -9.61714293e-03]],\n",
       "\n",
       "       [[ 1.01420045e+00, -3.74213023e-01, -5.47215559e-01, ...,\n",
       "          3.08422923e-02, -4.45360308e-02, -9.61714293e-03]],\n",
       "\n",
       "       [[ 1.53857934e+00, -2.25167246e-01,  4.45008530e-01, ...,\n",
       "          6.04008611e-02, -5.63934797e-02, -1.12370553e-03]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      2\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "      ..\n",
       "415    3\n",
       "416    3\n",
       "417    3\n",
       "418    3\n",
       "419    3\n",
       "Name: bug, Length: 420, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 3, 0, 0, 0, 2, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = Y_train.to_numpy()\n",
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.52329275, -0.17009031,  0.12845599, ..., -0.03488069,\n",
       "         -0.02853661, -0.02754137]],\n",
       "\n",
       "       [[ 1.65167769, -0.06325479,  0.36856482, ...,  0.00259279,\n",
       "         -0.02678534,  0.01689497]],\n",
       "\n",
       "       [[ 1.02228015, -0.14704317, -0.59470571, ...,  0.00630524,\n",
       "          0.01799199,  0.02629242]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 1.02751126, -0.1264845 , -0.5919174 , ...,  0.16025114,\n",
       "          0.095574  , -0.12832104]],\n",
       "\n",
       "       [[ 1.31800237, -0.51424357, -0.77553422, ...,  0.02272707,\n",
       "          0.09587535, -0.12832784]],\n",
       "\n",
       "       [[ 1.53449933,  0.03236187,  0.0688109 , ..., -0.01911218,\n",
       "          0.00320715, -0.03528819]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "      ..\n",
       "173    0\n",
       "174    0\n",
       "175    0\n",
       "176    1\n",
       "177    0\n",
       "Name: bug, Length: 178, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "       0, 3, 0, 0, 0, 0, 3, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,\n",
       "       0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y = Y_test.to_numpy()\n",
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Designing and initializing the model.\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape = (1,components), dropout = 0.2, return_sequences=True))\n",
    "model.add(LSTM(80, dropout = 0.2, return_sequences=True))\n",
    "model.add(LSTM(60, dropout = 0.2, return_sequences=False))\n",
    "model.add(Dense(1, activation = 'relu'))\n",
    "model.compile(loss = 'mse' , optimizer = 'adam' , metrics = ['mse', 'mae', tf.keras.metrics.RootMeanSquaredError(), tf.keras.metrics.MeanSquaredLogarithmicError()] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Designing and initializing the model.\n",
    "model = Sequential()\n",
    "model.add(Dense(15, activation = 'relu', input_dim = components))\n",
    "model.add(Dense(8, activation = 'relu'))\n",
    "model.add(Dense(5, activation = 'relu'))\n",
    "# Adding the output layer\n",
    "model.add(Dense(1, activation = 'relu'))\n",
    "model.compile(loss = 'mse' , optimizer = 'adam' , metrics = ['mse', 'mae', tf.keras.metrics.RootMeanSquaredError(), tf.keras.metrics.MeanSquaredLogarithmicError()] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 15)                240       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 128       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 45        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 419\n",
      "Trainable params: 419\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 15) for input Tensor(\"dense_input:0\", shape=(None, 15), dtype=float32), but it was called on an input with incompatible shape (None, 1, 15).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 15) for input Tensor(\"dense_input:0\", shape=(None, 15), dtype=float32), but it was called on an input with incompatible shape (None, 1, 15).\n",
      "4/4 [==============================] - 0s 796us/step - loss: 3.3469 - mse: 3.3469 - mae: 1.4750 - root_mean_squared_error: 1.8295 - mean_squared_logarithmic_error: 0.8261\n",
      "Epoch 2/100\n",
      "4/4 [==============================] - 0s 770us/step - loss: 3.2839 - mse: 3.2839 - mae: 1.4637 - root_mean_squared_error: 1.8122 - mean_squared_logarithmic_error: 0.7962\n",
      "Epoch 3/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 3.2223 - mse: 3.2223 - mae: 1.4530 - root_mean_squared_error: 1.7951 - mean_squared_logarithmic_error: 0.7682\n",
      "Epoch 4/100\n",
      "4/4 [==============================] - 0s 682us/step - loss: 3.1596 - mse: 3.1596 - mae: 1.4419 - root_mean_squared_error: 1.7775 - mean_squared_logarithmic_error: 0.7405\n",
      "Epoch 5/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 3.0974 - mse: 3.0974 - mae: 1.4303 - root_mean_squared_error: 1.7599 - mean_squared_logarithmic_error: 0.7136\n",
      "Epoch 6/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 3.0326 - mse: 3.0326 - mae: 1.4189 - root_mean_squared_error: 1.7414 - mean_squared_logarithmic_error: 0.6869\n",
      "Epoch 7/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 2.9654 - mse: 2.9654 - mae: 1.4056 - root_mean_squared_error: 1.7220 - mean_squared_logarithmic_error: 0.6597\n",
      "Epoch 8/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 2.8994 - mse: 2.8994 - mae: 1.3935 - root_mean_squared_error: 1.7028 - mean_squared_logarithmic_error: 0.6341\n",
      "Epoch 9/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 2.8306 - mse: 2.8306 - mae: 1.3802 - root_mean_squared_error: 1.6825 - mean_squared_logarithmic_error: 0.6084\n",
      "Epoch 10/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 2.7580 - mse: 2.7580 - mae: 1.3657 - root_mean_squared_error: 1.6607 - mean_squared_logarithmic_error: 0.5823\n",
      "Epoch 11/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 2.6806 - mse: 2.6806 - mae: 1.3500 - root_mean_squared_error: 1.6373 - mean_squared_logarithmic_error: 0.5557\n",
      "Epoch 12/100\n",
      "4/4 [==============================] - 0s 968us/step - loss: 2.6006 - mse: 2.6006 - mae: 1.3333 - root_mean_squared_error: 1.6127 - mean_squared_logarithmic_error: 0.5293\n",
      "Epoch 13/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 2.5175 - mse: 2.5175 - mae: 1.3150 - root_mean_squared_error: 1.5867 - mean_squared_logarithmic_error: 0.5027\n",
      "Epoch 14/100\n",
      "4/4 [==============================] - 0s 655us/step - loss: 2.4338 - mse: 2.4338 - mae: 1.2962 - root_mean_squared_error: 1.5601 - mean_squared_logarithmic_error: 0.4773\n",
      "Epoch 15/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.3480 - mse: 2.3480 - mae: 1.2767 - root_mean_squared_error: 1.5323 - mean_squared_logarithmic_error: 0.4525\n",
      "Epoch 16/100\n",
      "4/4 [==============================] - 0s 616us/step - loss: 2.2603 - mse: 2.2603 - mae: 1.2553 - root_mean_squared_error: 1.5034 - mean_squared_logarithmic_error: 0.4282\n",
      "Epoch 17/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.1713 - mse: 2.1713 - mae: 1.2340 - root_mean_squared_error: 1.4735 - mean_squared_logarithmic_error: 0.4052\n",
      "Epoch 18/100\n",
      "4/4 [==============================] - 0s 688us/step - loss: 2.0833 - mse: 2.0833 - mae: 1.2106 - root_mean_squared_error: 1.4434 - mean_squared_logarithmic_error: 0.3832\n",
      "Epoch 19/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.9935 - mse: 1.9935 - mae: 1.1863 - root_mean_squared_error: 1.4119 - mean_squared_logarithmic_error: 0.3622\n",
      "Epoch 20/100\n",
      "4/4 [==============================] - 0s 665us/step - loss: 1.9046 - mse: 1.9046 - mae: 1.1606 - root_mean_squared_error: 1.3801 - mean_squared_logarithmic_error: 0.3424\n",
      "Epoch 21/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.8144 - mse: 1.8144 - mae: 1.1325 - root_mean_squared_error: 1.3470 - mean_squared_logarithmic_error: 0.3233\n",
      "Epoch 22/100\n",
      "4/4 [==============================] - 0s 646us/step - loss: 1.7273 - mse: 1.7273 - mae: 1.1043 - root_mean_squared_error: 1.3143 - mean_squared_logarithmic_error: 0.3063\n",
      "Epoch 23/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.6386 - mse: 1.6386 - mae: 1.0755 - root_mean_squared_error: 1.2801 - mean_squared_logarithmic_error: 0.2904\n",
      "Epoch 24/100\n",
      "4/4 [==============================] - 0s 670us/step - loss: 1.5557 - mse: 1.5557 - mae: 1.0487 - root_mean_squared_error: 1.2473 - mean_squared_logarithmic_error: 0.2763\n",
      "Epoch 25/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.4759 - mse: 1.4759 - mae: 1.0215 - root_mean_squared_error: 1.2149 - mean_squared_logarithmic_error: 0.2642\n",
      "Epoch 26/100\n",
      "4/4 [==============================] - 0s 725us/step - loss: 1.4024 - mse: 1.4024 - mae: 0.9935 - root_mean_squared_error: 1.1842 - mean_squared_logarithmic_error: 0.2539\n",
      "Epoch 27/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.3296 - mse: 1.3296 - mae: 0.9673 - root_mean_squared_error: 1.1531 - mean_squared_logarithmic_error: 0.2446\n",
      "Epoch 28/100\n",
      "4/4 [==============================] - 0s 979us/step - loss: 1.2692 - mse: 1.2692 - mae: 0.9465 - root_mean_squared_error: 1.1266 - mean_squared_logarithmic_error: 0.2383\n",
      "Epoch 29/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.2108 - mse: 1.2108 - mae: 0.9304 - root_mean_squared_error: 1.1004 - mean_squared_logarithmic_error: 0.2325\n",
      "Epoch 30/100\n",
      "4/4 [==============================] - 0s 705us/step - loss: 1.1608 - mse: 1.1608 - mae: 0.9241 - root_mean_squared_error: 1.0774 - mean_squared_logarithmic_error: 0.2288\n",
      "Epoch 31/100\n",
      "4/4 [==============================] - 0s 728us/step - loss: 1.1155 - mse: 1.1155 - mae: 0.9192 - root_mean_squared_error: 1.0562 - mean_squared_logarithmic_error: 0.2261\n",
      "Epoch 32/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.0786 - mse: 1.0786 - mae: 0.9142 - root_mean_squared_error: 1.0386 - mean_squared_logarithmic_error: 0.2250\n",
      "Epoch 33/100\n",
      "4/4 [==============================] - 0s 649us/step - loss: 1.0468 - mse: 1.0468 - mae: 0.9086 - root_mean_squared_error: 1.0231 - mean_squared_logarithmic_error: 0.2244\n",
      "Epoch 34/100\n",
      "4/4 [==============================] - 0s 946us/step - loss: 1.0210 - mse: 1.0210 - mae: 0.9029 - root_mean_squared_error: 1.0104 - mean_squared_logarithmic_error: 0.2245\n",
      "Epoch 35/100\n",
      "4/4 [==============================] - 0s 672us/step - loss: 1.0007 - mse: 1.0007 - mae: 0.8974 - root_mean_squared_error: 1.0003 - mean_squared_logarithmic_error: 0.2251\n",
      "Epoch 36/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.9859 - mse: 0.9859 - mae: 0.8920 - root_mean_squared_error: 0.9929 - mean_squared_logarithmic_error: 0.2262\n",
      "Epoch 37/100\n",
      "4/4 [==============================] - 0s 732us/step - loss: 0.9726 - mse: 0.9726 - mae: 0.8862 - root_mean_squared_error: 0.9862 - mean_squared_logarithmic_error: 0.2265\n",
      "Epoch 38/100\n",
      "4/4 [==============================] - 0s 644us/step - loss: 0.9636 - mse: 0.9636 - mae: 0.8817 - root_mean_squared_error: 0.9816 - mean_squared_logarithmic_error: 0.2268\n",
      "Epoch 39/100\n",
      "4/4 [==============================] - 0s 881us/step - loss: 0.9553 - mse: 0.9553 - mae: 0.8771 - root_mean_squared_error: 0.9774 - mean_squared_logarithmic_error: 0.2266\n",
      "Epoch 40/100\n",
      "4/4 [==============================] - 0s 707us/step - loss: 0.9473 - mse: 0.9473 - mae: 0.8727 - root_mean_squared_error: 0.9733 - mean_squared_logarithmic_error: 0.2259\n",
      "Epoch 41/100\n",
      "4/4 [==============================] - 0s 918us/step - loss: 0.9404 - mse: 0.9404 - mae: 0.8683 - root_mean_squared_error: 0.9697 - mean_squared_logarithmic_error: 0.2253\n",
      "Epoch 42/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.9343 - mse: 0.9343 - mae: 0.8645 - root_mean_squared_error: 0.9666 - mean_squared_logarithmic_error: 0.2248\n",
      "Epoch 43/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.9279 - mse: 0.9279 - mae: 0.8606 - root_mean_squared_error: 0.9633 - mean_squared_logarithmic_error: 0.2237\n",
      "Epoch 44/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.9213 - mse: 0.9213 - mae: 0.8566 - root_mean_squared_error: 0.9599 - mean_squared_logarithmic_error: 0.2224\n",
      "Epoch 45/100\n",
      "4/4 [==============================] - 0s 693us/step - loss: 0.9151 - mse: 0.9151 - mae: 0.8526 - root_mean_squared_error: 0.9566 - mean_squared_logarithmic_error: 0.2214\n",
      "Epoch 46/100\n",
      "4/4 [==============================] - 0s 640us/step - loss: 0.9082 - mse: 0.9082 - mae: 0.8485 - root_mean_squared_error: 0.9530 - mean_squared_logarithmic_error: 0.2200\n",
      "Epoch 47/100\n",
      "4/4 [==============================] - 0s 654us/step - loss: 0.9023 - mse: 0.9023 - mae: 0.8448 - root_mean_squared_error: 0.9499 - mean_squared_logarithmic_error: 0.2185\n",
      "Epoch 48/100\n",
      "4/4 [==============================] - 0s 674us/step - loss: 0.8953 - mse: 0.8953 - mae: 0.8403 - root_mean_squared_error: 0.9462 - mean_squared_logarithmic_error: 0.2170\n",
      "Epoch 49/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.8880 - mse: 0.8880 - mae: 0.8354 - root_mean_squared_error: 0.9423 - mean_squared_logarithmic_error: 0.2156\n",
      "Epoch 50/100\n",
      "4/4 [==============================] - 0s 603us/step - loss: 0.8811 - mse: 0.8811 - mae: 0.8313 - root_mean_squared_error: 0.9387 - mean_squared_logarithmic_error: 0.2139\n",
      "Epoch 51/100\n",
      "4/4 [==============================] - 0s 675us/step - loss: 0.8753 - mse: 0.8753 - mae: 0.8276 - root_mean_squared_error: 0.9356 - mean_squared_logarithmic_error: 0.2125\n",
      "Epoch 52/100\n",
      "4/4 [==============================] - 0s 649us/step - loss: 0.8689 - mse: 0.8689 - mae: 0.8232 - root_mean_squared_error: 0.9321 - mean_squared_logarithmic_error: 0.2113\n",
      "Epoch 53/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.8623 - mse: 0.8623 - mae: 0.8186 - root_mean_squared_error: 0.9286 - mean_squared_logarithmic_error: 0.2101\n",
      "Epoch 54/100\n",
      "4/4 [==============================] - 0s 739us/step - loss: 0.8554 - mse: 0.8554 - mae: 0.8145 - root_mean_squared_error: 0.9249 - mean_squared_logarithmic_error: 0.2085\n",
      "Epoch 55/100\n",
      "4/4 [==============================] - 0s 849us/step - loss: 0.8495 - mse: 0.8495 - mae: 0.8109 - root_mean_squared_error: 0.9217 - mean_squared_logarithmic_error: 0.2068\n",
      "Epoch 56/100\n",
      "4/4 [==============================] - 0s 745us/step - loss: 0.8428 - mse: 0.8428 - mae: 0.8069 - root_mean_squared_error: 0.9180 - mean_squared_logarithmic_error: 0.2053\n",
      "Epoch 57/100\n",
      "4/4 [==============================] - 0s 724us/step - loss: 0.8362 - mse: 0.8362 - mae: 0.8031 - root_mean_squared_error: 0.9144 - mean_squared_logarithmic_error: 0.2038\n",
      "Epoch 58/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.8296 - mse: 0.8296 - mae: 0.7990 - root_mean_squared_error: 0.9108 - mean_squared_logarithmic_error: 0.2020\n",
      "Epoch 59/100\n",
      "4/4 [==============================] - 0s 833us/step - loss: 0.8238 - mse: 0.8238 - mae: 0.7954 - root_mean_squared_error: 0.9076 - mean_squared_logarithmic_error: 0.2004\n",
      "Epoch 60/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.8173 - mse: 0.8173 - mae: 0.7911 - root_mean_squared_error: 0.9041 - mean_squared_logarithmic_error: 0.1990\n",
      "Epoch 61/100\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.8107 - mse: 0.8107 - mae: 0.7864 - root_mean_squared_error: 0.9004 - mean_squared_logarithmic_error: 0.1980\n",
      "Epoch 62/100\n",
      "4/4 [==============================] - 0s 669us/step - loss: 0.8040 - mse: 0.8040 - mae: 0.7813 - root_mean_squared_error: 0.8967 - mean_squared_logarithmic_error: 0.1974\n",
      "Epoch 63/100\n",
      "4/4 [==============================] - 0s 692us/step - loss: 0.7973 - mse: 0.7973 - mae: 0.7761 - root_mean_squared_error: 0.8929 - mean_squared_logarithmic_error: 0.1971\n",
      "Epoch 64/100\n",
      "4/4 [==============================] - 0s 669us/step - loss: 0.7905 - mse: 0.7905 - mae: 0.7707 - root_mean_squared_error: 0.8891 - mean_squared_logarithmic_error: 0.1964\n",
      "Epoch 65/100\n",
      "4/4 [==============================] - 0s 829us/step - loss: 0.7839 - mse: 0.7839 - mae: 0.7654 - root_mean_squared_error: 0.8854 - mean_squared_logarithmic_error: 0.1955\n",
      "Epoch 66/100\n",
      "4/4 [==============================] - 0s 791us/step - loss: 0.7768 - mse: 0.7768 - mae: 0.7606 - root_mean_squared_error: 0.8813 - mean_squared_logarithmic_error: 0.1942\n",
      "Epoch 67/100\n",
      "4/4 [==============================] - 0s 853us/step - loss: 0.7694 - mse: 0.7694 - mae: 0.7560 - root_mean_squared_error: 0.8772 - mean_squared_logarithmic_error: 0.1924\n",
      "Epoch 68/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.7627 - mse: 0.7627 - mae: 0.7518 - root_mean_squared_error: 0.8733 - mean_squared_logarithmic_error: 0.1905\n",
      "Epoch 69/100\n",
      "4/4 [==============================] - 0s 678us/step - loss: 0.7553 - mse: 0.7553 - mae: 0.7468 - root_mean_squared_error: 0.8691 - mean_squared_logarithmic_error: 0.1888\n",
      "Epoch 70/100\n",
      "4/4 [==============================] - 0s 607us/step - loss: 0.7482 - mse: 0.7482 - mae: 0.7419 - root_mean_squared_error: 0.8650 - mean_squared_logarithmic_error: 0.1872\n",
      "Epoch 71/100\n",
      "4/4 [==============================] - 0s 738us/step - loss: 0.7406 - mse: 0.7406 - mae: 0.7372 - root_mean_squared_error: 0.8606 - mean_squared_logarithmic_error: 0.1853\n",
      "Epoch 72/100\n",
      "4/4 [==============================] - 0s 640us/step - loss: 0.7334 - mse: 0.7334 - mae: 0.7326 - root_mean_squared_error: 0.8564 - mean_squared_logarithmic_error: 0.1834\n",
      "Epoch 73/100\n",
      "4/4 [==============================] - 0s 684us/step - loss: 0.7259 - mse: 0.7259 - mae: 0.7275 - root_mean_squared_error: 0.8520 - mean_squared_logarithmic_error: 0.1819\n",
      "Epoch 74/100\n",
      "4/4 [==============================] - 0s 975us/step - loss: 0.7184 - mse: 0.7184 - mae: 0.7222 - root_mean_squared_error: 0.8476 - mean_squared_logarithmic_error: 0.1805\n",
      "Epoch 75/100\n",
      "4/4 [==============================] - 0s 644us/step - loss: 0.7107 - mse: 0.7107 - mae: 0.7169 - root_mean_squared_error: 0.8430 - mean_squared_logarithmic_error: 0.1787\n",
      "Epoch 76/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.7032 - mse: 0.7032 - mae: 0.7118 - root_mean_squared_error: 0.8386 - mean_squared_logarithmic_error: 0.1771\n",
      "Epoch 77/100\n",
      "4/4 [==============================] - 0s 708us/step - loss: 0.6957 - mse: 0.6957 - mae: 0.7066 - root_mean_squared_error: 0.8341 - mean_squared_logarithmic_error: 0.1755\n",
      "Epoch 78/100\n",
      "4/4 [==============================] - 0s 738us/step - loss: 0.6885 - mse: 0.6885 - mae: 0.7018 - root_mean_squared_error: 0.8298 - mean_squared_logarithmic_error: 0.1738\n",
      "Epoch 79/100\n",
      "4/4 [==============================] - 0s 741us/step - loss: 0.6807 - mse: 0.6807 - mae: 0.6970 - root_mean_squared_error: 0.8251 - mean_squared_logarithmic_error: 0.1717\n",
      "Epoch 80/100\n",
      "4/4 [==============================] - 0s 766us/step - loss: 0.6736 - mse: 0.6736 - mae: 0.6927 - root_mean_squared_error: 0.8208 - mean_squared_logarithmic_error: 0.1695\n",
      "Epoch 81/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.6665 - mse: 0.6665 - mae: 0.6880 - root_mean_squared_error: 0.8164 - mean_squared_logarithmic_error: 0.1675\n",
      "Epoch 82/100\n",
      "4/4 [==============================] - 0s 712us/step - loss: 0.6586 - mse: 0.6586 - mae: 0.6828 - root_mean_squared_error: 0.8116 - mean_squared_logarithmic_error: 0.1658\n",
      "Epoch 83/100\n",
      "4/4 [==============================] - 0s 681us/step - loss: 0.6506 - mse: 0.6506 - mae: 0.6764 - root_mean_squared_error: 0.8066 - mean_squared_logarithmic_error: 0.1646\n",
      "Epoch 84/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.6428 - mse: 0.6428 - mae: 0.6698 - root_mean_squared_error: 0.8017 - mean_squared_logarithmic_error: 0.1637\n",
      "Epoch 85/100\n",
      "4/4 [==============================] - 0s 660us/step - loss: 0.6343 - mse: 0.6343 - mae: 0.6629 - root_mean_squared_error: 0.7964 - mean_squared_logarithmic_error: 0.1626\n",
      "Epoch 86/100\n",
      "4/4 [==============================] - 0s 795us/step - loss: 0.6266 - mse: 0.6266 - mae: 0.6567 - root_mean_squared_error: 0.7916 - mean_squared_logarithmic_error: 0.1615\n",
      "Epoch 87/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 664us/step - loss: 0.6182 - mse: 0.6182 - mae: 0.6520 - root_mean_squared_error: 0.7863 - mean_squared_logarithmic_error: 0.1595\n",
      "Epoch 88/100\n",
      "4/4 [==============================] - 0s 926us/step - loss: 0.6109 - mse: 0.6109 - mae: 0.6489 - root_mean_squared_error: 0.7816 - mean_squared_logarithmic_error: 0.1570\n",
      "Epoch 89/100\n",
      "4/4 [==============================] - 0s 698us/step - loss: 0.6034 - mse: 0.6034 - mae: 0.6449 - root_mean_squared_error: 0.7768 - mean_squared_logarithmic_error: 0.1549\n",
      "Epoch 90/100\n",
      "4/4 [==============================] - 0s 983us/step - loss: 0.5957 - mse: 0.5957 - mae: 0.6395 - root_mean_squared_error: 0.7718 - mean_squared_logarithmic_error: 0.1532\n",
      "Epoch 91/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.5878 - mse: 0.5878 - mae: 0.6345 - root_mean_squared_error: 0.7667 - mean_squared_logarithmic_error: 0.1511\n",
      "Epoch 92/100\n",
      "4/4 [==============================] - 0s 675us/step - loss: 0.5804 - mse: 0.5804 - mae: 0.6296 - root_mean_squared_error: 0.7618 - mean_squared_logarithmic_error: 0.1494\n",
      "Epoch 93/100\n",
      "4/4 [==============================] - 0s 698us/step - loss: 0.5729 - mse: 0.5729 - mae: 0.6234 - root_mean_squared_error: 0.7569 - mean_squared_logarithmic_error: 0.1480\n",
      "Epoch 94/100\n",
      "4/4 [==============================] - 0s 763us/step - loss: 0.5652 - mse: 0.5652 - mae: 0.6163 - root_mean_squared_error: 0.7518 - mean_squared_logarithmic_error: 0.1469\n",
      "Epoch 95/100\n",
      "4/4 [==============================] - 0s 644us/step - loss: 0.5583 - mse: 0.5583 - mae: 0.6098 - root_mean_squared_error: 0.7472 - mean_squared_logarithmic_error: 0.1459\n",
      "Epoch 96/100\n",
      "4/4 [==============================] - 0s 843us/step - loss: 0.5512 - mse: 0.5512 - mae: 0.6036 - root_mean_squared_error: 0.7424 - mean_squared_logarithmic_error: 0.1448\n",
      "Epoch 97/100\n",
      "4/4 [==============================] - 0s 684us/step - loss: 0.5440 - mse: 0.5440 - mae: 0.5978 - root_mean_squared_error: 0.7376 - mean_squared_logarithmic_error: 0.1433\n",
      "Epoch 98/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.5366 - mse: 0.5366 - mae: 0.5922 - root_mean_squared_error: 0.7326 - mean_squared_logarithmic_error: 0.1417\n",
      "Epoch 99/100\n",
      "4/4 [==============================] - 0s 729us/step - loss: 0.5293 - mse: 0.5293 - mae: 0.5868 - root_mean_squared_error: 0.7275 - mean_squared_logarithmic_error: 0.1400\n",
      "Epoch 100/100\n",
      "4/4 [==============================] - 0s 856us/step - loss: 0.5216 - mse: 0.5216 - mae: 0.5804 - root_mean_squared_error: 0.7222 - mean_squared_logarithmic_error: 0.1383\n"
     ]
    }
   ],
   "source": [
    "# Fitting the model on training data.\n",
    "history = model.fit(train_x, train_y, epochs = 100, batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 15) for input Tensor(\"dense_input:0\", shape=(None, 15), dtype=float32), but it was called on an input with incompatible shape (None, 1, 15).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[1.1334407 ]],\n",
       "\n",
       "       [[1.7524495 ]],\n",
       "\n",
       "       [[0.9087959 ]],\n",
       "\n",
       "       [[1.4855871 ]],\n",
       "\n",
       "       [[1.6402361 ]],\n",
       "\n",
       "       [[1.4108789 ]],\n",
       "\n",
       "       [[1.5698905 ]],\n",
       "\n",
       "       [[0.2353195 ]],\n",
       "\n",
       "       [[0.6107452 ]],\n",
       "\n",
       "       [[2.699305  ]],\n",
       "\n",
       "       [[0.94542533]],\n",
       "\n",
       "       [[2.2298372 ]],\n",
       "\n",
       "       [[0.57242656]],\n",
       "\n",
       "       [[1.4197145 ]],\n",
       "\n",
       "       [[1.5444288 ]],\n",
       "\n",
       "       [[0.2353195 ]],\n",
       "\n",
       "       [[2.5671415 ]],\n",
       "\n",
       "       [[0.33354235]],\n",
       "\n",
       "       [[0.2353195 ]],\n",
       "\n",
       "       [[0.7105511 ]],\n",
       "\n",
       "       [[0.9231513 ]],\n",
       "\n",
       "       [[0.72738063]],\n",
       "\n",
       "       [[0.6096236 ]],\n",
       "\n",
       "       [[1.2254436 ]],\n",
       "\n",
       "       [[1.9677575 ]],\n",
       "\n",
       "       [[1.3487101 ]],\n",
       "\n",
       "       [[1.376359  ]],\n",
       "\n",
       "       [[0.7878943 ]],\n",
       "\n",
       "       [[1.2513418 ]],\n",
       "\n",
       "       [[0.2353195 ]],\n",
       "\n",
       "       [[0.2353195 ]],\n",
       "\n",
       "       [[0.39049965]],\n",
       "\n",
       "       [[0.36382502]],\n",
       "\n",
       "       [[1.459171  ]],\n",
       "\n",
       "       [[0.62933356]],\n",
       "\n",
       "       [[0.2353195 ]],\n",
       "\n",
       "       [[1.2060711 ]],\n",
       "\n",
       "       [[0.68176246]],\n",
       "\n",
       "       [[0.9900902 ]],\n",
       "\n",
       "       [[0.23532864]],\n",
       "\n",
       "       [[1.7340326 ]],\n",
       "\n",
       "       [[0.2353195 ]],\n",
       "\n",
       "       [[0.2355431 ]],\n",
       "\n",
       "       [[1.9716432 ]],\n",
       "\n",
       "       [[0.2353195 ]],\n",
       "\n",
       "       [[0.35717615]],\n",
       "\n",
       "       [[1.3749387 ]],\n",
       "\n",
       "       [[1.5321724 ]],\n",
       "\n",
       "       [[3.18674   ]],\n",
       "\n",
       "       [[0.96233183]],\n",
       "\n",
       "       [[0.8783299 ]],\n",
       "\n",
       "       [[0.2353195 ]],\n",
       "\n",
       "       [[0.38376123]],\n",
       "\n",
       "       [[0.2353195 ]],\n",
       "\n",
       "       [[0.5659591 ]],\n",
       "\n",
       "       [[0.2353195 ]],\n",
       "\n",
       "       [[1.9654334 ]],\n",
       "\n",
       "       [[0.58125967]],\n",
       "\n",
       "       [[0.2353195 ]],\n",
       "\n",
       "       [[0.26627895]],\n",
       "\n",
       "       [[0.8540315 ]],\n",
       "\n",
       "       [[1.742557  ]],\n",
       "\n",
       "       [[1.7076168 ]],\n",
       "\n",
       "       [[0.2353195 ]],\n",
       "\n",
       "       [[0.7884281 ]],\n",
       "\n",
       "       [[0.2353195 ]],\n",
       "\n",
       "       [[1.284303  ]],\n",
       "\n",
       "       [[1.6292224 ]],\n",
       "\n",
       "       [[0.5025636 ]],\n",
       "\n",
       "       [[0.5853452 ]],\n",
       "\n",
       "       [[1.167515  ]],\n",
       "\n",
       "       [[0.2353195 ]],\n",
       "\n",
       "       [[1.060039  ]],\n",
       "\n",
       "       [[0.45453045]],\n",
       "\n",
       "       [[1.6762824 ]],\n",
       "\n",
       "       [[1.3989344 ]],\n",
       "\n",
       "       [[1.2913864 ]],\n",
       "\n",
       "       [[0.2353195 ]],\n",
       "\n",
       "       [[2.0317385 ]],\n",
       "\n",
       "       [[0.57363117]],\n",
       "\n",
       "       [[3.0982285 ]],\n",
       "\n",
       "       [[0.2353195 ]],\n",
       "\n",
       "       [[0.2353195 ]],\n",
       "\n",
       "       [[0.27141827]],\n",
       "\n",
       "       [[1.6221902 ]],\n",
       "\n",
       "       [[0.50918335]],\n",
       "\n",
       "       [[0.23919457]],\n",
       "\n",
       "       [[0.2353195 ]],\n",
       "\n",
       "       [[1.309413  ]],\n",
       "\n",
       "       [[1.5570164 ]],\n",
       "\n",
       "       [[1.6461079 ]],\n",
       "\n",
       "       [[1.9247446 ]],\n",
       "\n",
       "       [[0.30881065]],\n",
       "\n",
       "       [[1.5679178 ]],\n",
       "\n",
       "       [[1.532896  ]],\n",
       "\n",
       "       [[1.058572  ]],\n",
       "\n",
       "       [[0.2353195 ]],\n",
       "\n",
       "       [[0.2353195 ]],\n",
       "\n",
       "       [[1.0298952 ]],\n",
       "\n",
       "       [[0.2353195 ]],\n",
       "\n",
       "       [[1.3706882 ]],\n",
       "\n",
       "       [[0.2353195 ]],\n",
       "\n",
       "       [[0.5617478 ]],\n",
       "\n",
       "       [[1.063484  ]],\n",
       "\n",
       "       [[0.2353195 ]],\n",
       "\n",
       "       [[1.5953107 ]],\n",
       "\n",
       "       [[0.74596757]],\n",
       "\n",
       "       [[0.2353195 ]],\n",
       "\n",
       "       [[0.27986515]],\n",
       "\n",
       "       [[2.7025707 ]],\n",
       "\n",
       "       [[1.119539  ]],\n",
       "\n",
       "       [[1.2292523 ]],\n",
       "\n",
       "       [[0.2353195 ]],\n",
       "\n",
       "       [[0.8389107 ]],\n",
       "\n",
       "       [[1.2785699 ]],\n",
       "\n",
       "       [[1.2616191 ]],\n",
       "\n",
       "       [[2.5215838 ]],\n",
       "\n",
       "       [[2.0812223 ]],\n",
       "\n",
       "       [[0.2353195 ]],\n",
       "\n",
       "       [[0.7127176 ]],\n",
       "\n",
       "       [[0.2353195 ]],\n",
       "\n",
       "       [[0.55731535]],\n",
       "\n",
       "       [[1.8365154 ]],\n",
       "\n",
       "       [[1.615484  ]],\n",
       "\n",
       "       [[0.2353195 ]],\n",
       "\n",
       "       [[0.98018605]],\n",
       "\n",
       "       [[0.2353195 ]],\n",
       "\n",
       "       [[0.2353195 ]],\n",
       "\n",
       "       [[0.9689154 ]],\n",
       "\n",
       "       [[2.629374  ]],\n",
       "\n",
       "       [[0.8184968 ]],\n",
       "\n",
       "       [[1.185427  ]],\n",
       "\n",
       "       [[0.2353195 ]],\n",
       "\n",
       "       [[1.1161737 ]],\n",
       "\n",
       "       [[2.765132  ]],\n",
       "\n",
       "       [[1.258498  ]],\n",
       "\n",
       "       [[0.75852484]],\n",
       "\n",
       "       [[0.2353195 ]],\n",
       "\n",
       "       [[1.2470572 ]],\n",
       "\n",
       "       [[1.6360743 ]],\n",
       "\n",
       "       [[0.6017748 ]],\n",
       "\n",
       "       [[1.2880671 ]],\n",
       "\n",
       "       [[0.82391715]],\n",
       "\n",
       "       [[0.2353195 ]],\n",
       "\n",
       "       [[0.9766932 ]],\n",
       "\n",
       "       [[0.2353195 ]],\n",
       "\n",
       "       [[0.2353195 ]],\n",
       "\n",
       "       [[0.8606038 ]],\n",
       "\n",
       "       [[1.267535  ]],\n",
       "\n",
       "       [[0.2686631 ]],\n",
       "\n",
       "       [[0.9885866 ]],\n",
       "\n",
       "       [[1.0733123 ]],\n",
       "\n",
       "       [[0.7935607 ]],\n",
       "\n",
       "       [[0.2353195 ]],\n",
       "\n",
       "       [[1.4482131 ]],\n",
       "\n",
       "       [[0.2353195 ]],\n",
       "\n",
       "       [[0.29782718]],\n",
       "\n",
       "       [[1.423789  ]],\n",
       "\n",
       "       [[1.2671645 ]],\n",
       "\n",
       "       [[1.9309654 ]],\n",
       "\n",
       "       [[1.8786871 ]],\n",
       "\n",
       "       [[0.7669864 ]],\n",
       "\n",
       "       [[0.9177943 ]],\n",
       "\n",
       "       [[1.7290382 ]],\n",
       "\n",
       "       [[1.1220815 ]],\n",
       "\n",
       "       [[1.5623703 ]],\n",
       "\n",
       "       [[0.96325296]],\n",
       "\n",
       "       [[1.1197553 ]],\n",
       "\n",
       "       [[0.2353195 ]],\n",
       "\n",
       "       [[0.7040488 ]],\n",
       "\n",
       "       [[0.2353195 ]],\n",
       "\n",
       "       [[0.27413565]],\n",
       "\n",
       "       [[0.6553423 ]],\n",
       "\n",
       "       [[0.2353195 ]],\n",
       "\n",
       "       [[1.6297472 ]],\n",
       "\n",
       "       [[1.7776423 ]],\n",
       "\n",
       "       [[0.8327549 ]],\n",
       "\n",
       "       [[0.52625203]]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the predictions.\n",
    "predictions_y = model.predict(test_x)\n",
    "predictions_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.]],\n",
       "\n",
       "       [[2.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[2.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[2.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[3.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[2.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[2.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[3.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[2.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[2.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[2.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[2.]],\n",
       "\n",
       "       [[3.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[2.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[2.]],\n",
       "\n",
       "       [[2.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[2.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[2.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[2.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[3.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[2.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[2.]],\n",
       "\n",
       "       [[2.]],\n",
       "\n",
       "       [[2.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[2.]],\n",
       "\n",
       "       [[2.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[2.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[3.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[3.]],\n",
       "\n",
       "       [[2.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[2.]],\n",
       "\n",
       "       [[2.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[3.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[3.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[2.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[2.]],\n",
       "\n",
       "       [[2.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[2.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[2.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[0.]],\n",
       "\n",
       "       [[2.]],\n",
       "\n",
       "       [[2.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rounding off the predictions to nearest\n",
    "#integer as count of bugs is an integer.\n",
    "predictions_y_round = np.rint(predictions_y)\n",
    "predictions_y_round "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 1, 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Viewing the number of predictions.\n",
    "predictions_y_round.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[169.]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the sum of all the predictions obtained to used while obtaining FPA\n",
    "s = 0\n",
    "for  t in range(predictions_y_round.shape[0]):\n",
    "    s+=predictions_y_round[t]\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.48361152]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtaining the value of FPA metric for the model\n",
    "Fpa = 0\n",
    "for  t in range(predictions_y_round.shape[0]):\n",
    "        x = 0\n",
    "        for j in range( predictions_y_round.shape[0]-t+1, predictions_y_round.shape[0]):\n",
    "               x = x + predictions_y_round[j]\n",
    "        \n",
    "        x = (x/s)/predictions_y_round.shape[0]\n",
    "        Fpa = Fpa + x\n",
    "Fpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49200496]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtaining the value of CLC metric for the model\n",
    "previous_obtained = predictions_y_round[predictions_y_round.shape[0] - 1]/s\n",
    "\n",
    "CLC = 0\n",
    "for i in range(predictions_y_round.shape[0]):\n",
    "    if(i==0):\n",
    "        CLC += 0 + previous_obtained\n",
    "    else:\n",
    "        additional = (predictions_y_round[predictions_y_round.shape[0] - 1 - i])/s\n",
    "        CLC += 2*previous_obtained + additional\n",
    "        previous_obtained += additional\n",
    "        \n",
    "CLC/=(2*predictions_y_round.shape[0])\n",
    "CLC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 15) for input Tensor(\"dense_input:0\", shape=(None, 15), dtype=float32), but it was called on an input with incompatible shape (None, 1, 15).\n",
      "6/6 [==============================] - 0s 581us/step - loss: 1.2139 - mse: 1.2139 - mae: 0.8938 - root_mean_squared_error: 1.1018 - mean_squared_logarithmic_error: 0.4127\n",
      "dict_keys(['loss', 'mse', 'mae', 'root_mean_squared_error', 'mean_squared_logarithmic_error'])\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 15) for input Tensor(\"dense_input:0\", shape=(None, 15), dtype=float32), but it was called on an input with incompatible shape (178, 1, 15).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['loss',\n",
       " 'mse',\n",
       " 'mae',\n",
       " 'root_mean_squared_error',\n",
       " 'mean_squared_logarithmic_error']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting direct metric results using the metrics given to model.\n",
    "score = model.evaluate(test_x, test_y)\n",
    "print(history.history.keys())\n",
    "model.test_on_batch(test_x, test_y)\n",
    "model.metrics_names\n",
    "#print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\n",
    "#print(\"\\n%s: %.2f%%\" % (model.metrics_names[2], score[2]*100))\n",
    "#print(\"\\n%s: %.2f%%\" % (model.metrics_names[3], score[3]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/bavanya/Downloads/deb_packages/home/bavanya/Desktop/venv_python/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /home/bavanya/Downloads/deb_packages/home/bavanya/Desktop/venv_python/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: /home/bavanya/Desktop/6thSem/BTP/regression_PROMISE_dataset/saved_models/ant1.3_ant1.4_model3/ant1.3_ant1.4_model3_1/assets\n"
     ]
    }
   ],
   "source": [
    "# Saving the model\n",
    "model_id = 7\n",
    "path_to_save = '/home/bavanya/Desktop/6thSem/BTP/regression_PROMISE_dataset/saved_models/ant1.3_ant1.4_model3/ant1.3_ant1.4_model3_1'\n",
    "model.save(path_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the results to csv file.\n",
    "heading = ['model_id', 'train_data_name', 'test_data_name'] + model.metrics_names + ['fpa', 'clc']\n",
    "score = [model_id, train_data_name, test_data_name] + score + [float(Fpa) , float(CLC)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_id',\n",
       " 'train_data_name',\n",
       " 'test_data_name',\n",
       " 'loss',\n",
       " 'mse',\n",
       " 'mae',\n",
       " 'root_mean_squared_error',\n",
       " 'mean_squared_logarithmic_error',\n",
       " 'fpa',\n",
       " 'clc']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7,\n",
       " 'ant-1.3',\n",
       " 'ant-1.4',\n",
       " 1.2139216661453247,\n",
       " 1.2139216661453247,\n",
       " 0.893781840801239,\n",
       " 1.1017811298370361,\n",
       " 0.41268062591552734,\n",
       " 0.483611524105072,\n",
       " 0.49200496077537537]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the results to csv file.\n",
    "with open(path_to_save + '_metric_results.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(heading)\n",
    "    writer.writerow(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model and to make sure that the model is saved properly.\n",
    "model_loaded = load_model(path_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
