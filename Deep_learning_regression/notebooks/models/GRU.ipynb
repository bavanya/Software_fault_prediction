{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MID 25\n",
    "### Taking ant1.3 as training data.\n",
    "### ant1.4 as testing data.\n",
    "### Min max scaling done to few columns: ['wmc', 'dit', 'noc', 'cbo', 'rfc', 'lcom', 'ca', 'ce', 'npm', 'lcom3', 'loc', 'dam', 'moa', 'mfa', 'cam', 'ic', 'cbm', 'amc', 'max_cc', 'avg_cc']\n",
    "### PCA used for feature reduction, n_components = 10.\n",
    "### Oversampling and smote methods used to increase size of training data.\n",
    "### GRU model used, model type is 4 as per the BTP documentation spreadsheet.\n",
    "### np.rint() used on predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time \n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from keras.layers import GRU, Dense\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data. \n",
    "train_data_path = \"/home/bavanya/Desktop/6thSem/BTP/regression_PROMISE_dataset/datasets/ant-1.3.csv\"\n",
    "test_data_path = \"/home/bavanya/Desktop/6thSem/BTP/regression_PROMISE_dataset/datasets/ant-1.4.csv\"\n",
    "train_data_name = \"ant-1.3\"\n",
    "test_data_name = \"ant-1.4\"\n",
    "ant_1_3 = pd.read_csv(train_data_path)\n",
    "ant_1_4 = pd.read_csv(test_data_path)\n",
    "files = [\"/home/bavanya/Desktop/6thSem/BTP/regression_PROMISE_dataset/datasets/ant-1.3.csv\", \"/home/bavanya/Desktop/6thSem/BTP/regression_PROMISE_dataset/datasets/ant-1.4.csv\"]\n",
    "combined_data = pd.concat(map(pd.read_csv, files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Min Max Scaling.\n",
    "scaler = MinMaxScaler()\n",
    "MinMaxScaler(copy=True, feature_range=(0, 1))\n",
    "cols_to_norm = ['wmc', 'dit', 'noc', 'cbo', 'rfc', 'lcom', 'ca', 'ce', 'npm', 'lcom3', 'loc', 'dam', 'moa', 'mfa', 'cam', 'ic', 'cbm', 'amc', 'max_cc', 'avg_cc']\n",
    "combined_data[cols_to_norm] = MinMaxScaler().fit_transform(combined_data[cols_to_norm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>version</th>\n",
       "      <th>name.1</th>\n",
       "      <th>wmc</th>\n",
       "      <th>dit</th>\n",
       "      <th>noc</th>\n",
       "      <th>cbo</th>\n",
       "      <th>rfc</th>\n",
       "      <th>lcom</th>\n",
       "      <th>ca</th>\n",
       "      <th>...</th>\n",
       "      <th>dam</th>\n",
       "      <th>moa</th>\n",
       "      <th>mfa</th>\n",
       "      <th>cam</th>\n",
       "      <th>ic</th>\n",
       "      <th>cbm</th>\n",
       "      <th>amc</th>\n",
       "      <th>max_cc</th>\n",
       "      <th>avg_cc</th>\n",
       "      <th>bug</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ant</td>\n",
       "      <td>1.3</td>\n",
       "      <td>org.apache.tools.ant.taskdefs.ExecuteOn</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.102941</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.011856</td>\n",
       "      <td>0.014815</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.885057</td>\n",
       "      <td>0.232323</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.165951</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>0.209085</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ant</td>\n",
       "      <td>1.3</td>\n",
       "      <td>org.apache.tools.ant.DefaultLogger</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.163265</td>\n",
       "      <td>0.020033</td>\n",
       "      <td>0.029630</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080979</td>\n",
       "      <td>0.171429</td>\n",
       "      <td>0.269903</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ant</td>\n",
       "      <td>1.3</td>\n",
       "      <td>org.apache.tools.ant.taskdefs.TaskOutputStream</td>\n",
       "      <td>0.038961</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.007353</td>\n",
       "      <td>0.045918</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.083267</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.109529</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ant</td>\n",
       "      <td>1.3</td>\n",
       "      <td>org.apache.tools.ant.taskdefs.Cvs</td>\n",
       "      <td>0.155844</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.088235</td>\n",
       "      <td>0.188776</td>\n",
       "      <td>0.013083</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115693</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>0.232742</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ant</td>\n",
       "      <td>1.3</td>\n",
       "      <td>org.apache.tools.ant.taskdefs.Copyfile</td>\n",
       "      <td>0.077922</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.880952</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.100881</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.136898</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>ant</td>\n",
       "      <td>1.4</td>\n",
       "      <td>org.apache.tools.ant.TaskAdapter</td>\n",
       "      <td>0.064935</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.051471</td>\n",
       "      <td>0.096939</td>\n",
       "      <td>0.001635</td>\n",
       "      <td>0.029630</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.138351</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.131428</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>ant</td>\n",
       "      <td>1.4</td>\n",
       "      <td>org.apache.tools.ant.taskdefs.rmic.DefaultRmic...</td>\n",
       "      <td>0.220779</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.110294</td>\n",
       "      <td>0.336735</td>\n",
       "      <td>0.036795</td>\n",
       "      <td>0.029630</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.197917</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.172373</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.396221</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>ant</td>\n",
       "      <td>1.4</td>\n",
       "      <td>org.apache.tools.ant.IntrospectionHelper</td>\n",
       "      <td>0.298701</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.227941</td>\n",
       "      <td>0.418367</td>\n",
       "      <td>0.082175</td>\n",
       "      <td>0.162963</td>\n",
       "      <td>...</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.219724</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.378561</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>ant</td>\n",
       "      <td>1.4</td>\n",
       "      <td>org.apache.tools.ant.taskdefs.compilers.Defaul...</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.161765</td>\n",
       "      <td>0.438776</td>\n",
       "      <td>0.006132</td>\n",
       "      <td>0.059259</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.373390</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.761689</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>ant</td>\n",
       "      <td>1.4</td>\n",
       "      <td>org.apache.tools.ant.NoBannerLogger</td>\n",
       "      <td>0.051948</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.022059</td>\n",
       "      <td>0.081633</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.064852</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.287498</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>303 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    name  version                                             name.1  \\\n",
       "0    ant      1.3            org.apache.tools.ant.taskdefs.ExecuteOn   \n",
       "1    ant      1.3                 org.apache.tools.ant.DefaultLogger   \n",
       "2    ant      1.3     org.apache.tools.ant.taskdefs.TaskOutputStream   \n",
       "3    ant      1.3                  org.apache.tools.ant.taskdefs.Cvs   \n",
       "4    ant      1.3             org.apache.tools.ant.taskdefs.Copyfile   \n",
       "..   ...      ...                                                ...   \n",
       "173  ant      1.4                   org.apache.tools.ant.TaskAdapter   \n",
       "174  ant      1.4  org.apache.tools.ant.taskdefs.rmic.DefaultRmic...   \n",
       "175  ant      1.4           org.apache.tools.ant.IntrospectionHelper   \n",
       "176  ant      1.4  org.apache.tools.ant.taskdefs.compilers.Defaul...   \n",
       "177  ant      1.4                org.apache.tools.ant.NoBannerLogger   \n",
       "\n",
       "          wmc  dit    noc       cbo       rfc      lcom        ca  ...  \\\n",
       "0    0.142857  0.6  0.050  0.102941  0.214286  0.011856  0.014815  ...   \n",
       "1    0.181818  0.0  0.025  0.058824  0.163265  0.020033  0.029630  ...   \n",
       "2    0.038961  0.2  0.000  0.007353  0.045918  0.000000  0.000000  ...   \n",
       "3    0.155844  0.4  0.000  0.088235  0.188776  0.013083  0.000000  ...   \n",
       "4    0.077922  0.4  0.000  0.029412  0.107143  0.000409  0.000000  ...   \n",
       "..        ...  ...    ...       ...       ...       ...       ...  ...   \n",
       "173  0.064935  0.4  0.000  0.051471  0.096939  0.001635  0.029630  ...   \n",
       "174  0.220779  0.0  0.075  0.110294  0.336735  0.036795  0.029630  ...   \n",
       "175  0.298701  0.0  0.000  0.227941  0.418367  0.082175  0.162963  ...   \n",
       "176  0.142857  0.0  0.200  0.161765  0.438776  0.006132  0.059259  ...   \n",
       "177  0.051948  0.2  0.000  0.022059  0.081633  0.000000  0.000000  ...   \n",
       "\n",
       "          dam       moa       mfa       cam    ic       cbm       amc  \\\n",
       "0    1.000000  0.111111  0.885057  0.232323  0.75  0.363636  0.165951   \n",
       "1    1.000000  0.000000  0.000000  0.307692  0.00  0.000000  0.080979   \n",
       "2    1.000000  0.111111  0.714286  0.666667  0.25  0.090909  0.083267   \n",
       "3    1.000000  0.111111  0.770833  0.458333  0.00  0.000000  0.115693   \n",
       "4    1.000000  0.000000  0.880952  0.416667  0.50  0.181818  0.100881   \n",
       "..        ...       ...       ...       ...   ...       ...       ...   \n",
       "173  0.000000  0.000000  0.902439  0.400000  0.25  0.090909  0.138351   \n",
       "174  1.000000  0.222222  0.000000  0.197917  0.00  0.000000  0.172373   \n",
       "175  0.444444  0.000000  0.000000  0.318182  0.00  0.000000  0.219724   \n",
       "176  1.000000  0.777778  0.000000  0.266667  0.00  0.000000  0.373390   \n",
       "177  1.000000  0.000000  0.842105  0.875000  0.25  0.090909  0.064852   \n",
       "\n",
       "       max_cc    avg_cc  bug  \n",
       "0    0.085714  0.209085    0  \n",
       "1    0.171429  0.269903    2  \n",
       "2    0.028571  0.109529    0  \n",
       "3    0.085714  0.232742    0  \n",
       "4    0.028571  0.136898    0  \n",
       "..        ...       ...  ...  \n",
       "173  0.028571  0.131428    0  \n",
       "174  0.400000  0.396221    0  \n",
       "175  0.685714  0.378561    0  \n",
       "176  0.628571  0.761689    1  \n",
       "177  0.142857  0.287498    0  \n",
       "\n",
       "[303 rows x 24 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define transform\n",
    "components = 10\n",
    "pca = PCA(n_components=components)\n",
    "# prepare transform on dataset\n",
    "pca.fit(combined_data[cols_to_norm])\n",
    "# apply transform to dataset\n",
    "transformed = pca.transform(combined_data[cols_to_norm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.818904</td>\n",
       "      <td>0.255541</td>\n",
       "      <td>0.173010</td>\n",
       "      <td>-0.085265</td>\n",
       "      <td>-0.298426</td>\n",
       "      <td>-0.041536</td>\n",
       "      <td>-0.029952</td>\n",
       "      <td>0.060052</td>\n",
       "      <td>-0.061844</td>\n",
       "      <td>0.129277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.211850</td>\n",
       "      <td>-0.596714</td>\n",
       "      <td>-0.058649</td>\n",
       "      <td>-0.060459</td>\n",
       "      <td>-0.059547</td>\n",
       "      <td>-0.063873</td>\n",
       "      <td>-0.088019</td>\n",
       "      <td>0.048838</td>\n",
       "      <td>0.136331</td>\n",
       "      <td>-0.023472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.310718</td>\n",
       "      <td>-0.021222</td>\n",
       "      <td>-0.538803</td>\n",
       "      <td>-0.015225</td>\n",
       "      <td>-0.015308</td>\n",
       "      <td>0.025091</td>\n",
       "      <td>0.105172</td>\n",
       "      <td>-0.187430</td>\n",
       "      <td>-0.136162</td>\n",
       "      <td>-0.004927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.328366</td>\n",
       "      <td>0.034184</td>\n",
       "      <td>-0.105838</td>\n",
       "      <td>-0.033729</td>\n",
       "      <td>0.314725</td>\n",
       "      <td>-0.066018</td>\n",
       "      <td>-0.185431</td>\n",
       "      <td>0.118595</td>\n",
       "      <td>-0.020596</td>\n",
       "      <td>-0.020034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.516435</td>\n",
       "      <td>0.234332</td>\n",
       "      <td>-0.202375</td>\n",
       "      <td>-0.122822</td>\n",
       "      <td>-0.155654</td>\n",
       "      <td>-0.028101</td>\n",
       "      <td>-0.014558</td>\n",
       "      <td>-0.023515</td>\n",
       "      <td>0.081734</td>\n",
       "      <td>0.039900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>0.126544</td>\n",
       "      <td>0.701796</td>\n",
       "      <td>0.109947</td>\n",
       "      <td>-0.104515</td>\n",
       "      <td>0.057198</td>\n",
       "      <td>-0.305038</td>\n",
       "      <td>0.278453</td>\n",
       "      <td>-0.261710</td>\n",
       "      <td>-0.178725</td>\n",
       "      <td>-0.005567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>0.071521</td>\n",
       "      <td>-0.725060</td>\n",
       "      <td>0.254031</td>\n",
       "      <td>0.156662</td>\n",
       "      <td>0.066015</td>\n",
       "      <td>-0.159664</td>\n",
       "      <td>-0.159516</td>\n",
       "      <td>0.152804</td>\n",
       "      <td>-0.060837</td>\n",
       "      <td>0.065179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>0.276368</td>\n",
       "      <td>-0.485600</td>\n",
       "      <td>0.763979</td>\n",
       "      <td>0.403732</td>\n",
       "      <td>0.309357</td>\n",
       "      <td>-0.081984</td>\n",
       "      <td>-0.039603</td>\n",
       "      <td>-0.037146</td>\n",
       "      <td>-0.153829</td>\n",
       "      <td>0.234048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>-0.023641</td>\n",
       "      <td>-0.851754</td>\n",
       "      <td>0.470180</td>\n",
       "      <td>0.584599</td>\n",
       "      <td>0.163868</td>\n",
       "      <td>-0.184260</td>\n",
       "      <td>-0.144292</td>\n",
       "      <td>0.435575</td>\n",
       "      <td>-0.433720</td>\n",
       "      <td>0.035253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>-0.305827</td>\n",
       "      <td>0.130311</td>\n",
       "      <td>-0.516061</td>\n",
       "      <td>0.225862</td>\n",
       "      <td>0.071176</td>\n",
       "      <td>0.149595</td>\n",
       "      <td>0.066592</td>\n",
       "      <td>-0.080964</td>\n",
       "      <td>-0.008366</td>\n",
       "      <td>-0.034391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>303 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0   -0.818904  0.255541  0.173010 -0.085265 -0.298426 -0.041536 -0.029952   \n",
       "1    0.211850 -0.596714 -0.058649 -0.060459 -0.059547 -0.063873 -0.088019   \n",
       "2   -0.310718 -0.021222 -0.538803 -0.015225 -0.015308  0.025091  0.105172   \n",
       "3   -0.328366  0.034184 -0.105838 -0.033729  0.314725 -0.066018 -0.185431   \n",
       "4   -0.516435  0.234332 -0.202375 -0.122822 -0.155654 -0.028101 -0.014558   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "298  0.126544  0.701796  0.109947 -0.104515  0.057198 -0.305038  0.278453   \n",
       "299  0.071521 -0.725060  0.254031  0.156662  0.066015 -0.159664 -0.159516   \n",
       "300  0.276368 -0.485600  0.763979  0.403732  0.309357 -0.081984 -0.039603   \n",
       "301 -0.023641 -0.851754  0.470180  0.584599  0.163868 -0.184260 -0.144292   \n",
       "302 -0.305827  0.130311 -0.516061  0.225862  0.071176  0.149595  0.066592   \n",
       "\n",
       "            7         8         9  \n",
       "0    0.060052 -0.061844  0.129277  \n",
       "1    0.048838  0.136331 -0.023472  \n",
       "2   -0.187430 -0.136162 -0.004927  \n",
       "3    0.118595 -0.020596 -0.020034  \n",
       "4   -0.023515  0.081734  0.039900  \n",
       "..        ...       ...       ...  \n",
       "298 -0.261710 -0.178725 -0.005567  \n",
       "299  0.152804 -0.060837  0.065179  \n",
       "300 -0.037146 -0.153829  0.234048  \n",
       "301  0.435575 -0.433720  0.035253  \n",
       "302 -0.080964 -0.008366 -0.034391  \n",
       "\n",
       "[303 rows x 10 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed = pd.DataFrame(transformed)\n",
    "transformed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_index_list = list(range(ant_1_3.shape[0]))\n",
    "train_data_index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 200,\n",
       " 201,\n",
       " 202,\n",
       " 203,\n",
       " 204,\n",
       " 205,\n",
       " 206,\n",
       " 207,\n",
       " 208,\n",
       " 209,\n",
       " 210,\n",
       " 211,\n",
       " 212,\n",
       " 213,\n",
       " 214,\n",
       " 215,\n",
       " 216,\n",
       " 217,\n",
       " 218,\n",
       " 219,\n",
       " 220,\n",
       " 221,\n",
       " 222,\n",
       " 223,\n",
       " 224,\n",
       " 225,\n",
       " 226,\n",
       " 227,\n",
       " 228,\n",
       " 229,\n",
       " 230,\n",
       " 231,\n",
       " 232,\n",
       " 233,\n",
       " 234,\n",
       " 235,\n",
       " 236,\n",
       " 237,\n",
       " 238,\n",
       " 239,\n",
       " 240,\n",
       " 241,\n",
       " 242,\n",
       " 243,\n",
       " 244,\n",
       " 245,\n",
       " 246,\n",
       " 247,\n",
       " 248,\n",
       " 249,\n",
       " 250,\n",
       " 251,\n",
       " 252,\n",
       " 253,\n",
       " 254,\n",
       " 255,\n",
       " 256,\n",
       " 257,\n",
       " 258,\n",
       " 259,\n",
       " 260,\n",
       " 261,\n",
       " 262,\n",
       " 263,\n",
       " 264,\n",
       " 265,\n",
       " 266,\n",
       " 267,\n",
       " 268,\n",
       " 269,\n",
       " 270,\n",
       " 271,\n",
       " 272,\n",
       " 273,\n",
       " 274,\n",
       " 275,\n",
       " 276,\n",
       " 277,\n",
       " 278,\n",
       " 279,\n",
       " 280,\n",
       " 281,\n",
       " 282,\n",
       " 283,\n",
       " 284,\n",
       " 285,\n",
       " 286,\n",
       " 287,\n",
       " 288,\n",
       " 289,\n",
       " 290,\n",
       " 291,\n",
       " 292,\n",
       " 293,\n",
       " 294,\n",
       " 295,\n",
       " 296,\n",
       " 297,\n",
       " 298,\n",
       " 299,\n",
       " 300,\n",
       " 301,\n",
       " 302]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_index_list = list(range(ant_1_3.shape[0], ant_1_3.shape[0] + ant_1_4.shape[0]))\n",
    "test_data_index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = transformed[transformed.index.isin(train_data_index_list)]\n",
    "X_train = np.array(X_train)\n",
    "\n",
    "X_test = transformed[transformed.index.isin(test_data_index_list)]\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = combined_data[transformed.index.isin(train_data_index_list)]\n",
    "Y_train = Y_train['bug']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = combined_data[transformed.index.isin(test_data_index_list)]\n",
    "Y_test = Y_test['bug']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying over sampling and SMOTE to training data for augmentation.\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "X_train, Y_train = ros.fit_resample(X_train, Y_train)\n",
    "\n",
    "smt = SMOTE()\n",
    "X_train, Y_train = smt.fit_resample(X_train, Y_train)\n",
    "\n",
    "train_x = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "test_x = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.81890418,  0.25554076,  0.17301012, ...,  0.06005162,\n",
       "         -0.06184364,  0.12927686]],\n",
       "\n",
       "       [[ 0.21184961, -0.59671433, -0.0586493 , ...,  0.04883811,\n",
       "          0.13633057, -0.02347176]],\n",
       "\n",
       "       [[-0.31071828, -0.02122237, -0.53880307, ..., -0.18743043,\n",
       "         -0.13616219, -0.00492705]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.13335335, -0.65412279,  0.11619592, ...,  0.01685192,\n",
       "          0.05180648,  0.15503896]],\n",
       "\n",
       "       [[ 0.13335335, -0.65412279,  0.11619592, ...,  0.01685192,\n",
       "          0.05180648,  0.15503896]],\n",
       "\n",
       "       [[-0.64581609,  0.16459048, -0.08562776, ..., -0.01983011,\n",
       "         -0.08398178,  0.10540274]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      2\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "      ..\n",
       "415    3\n",
       "416    3\n",
       "417    3\n",
       "418    3\n",
       "419    3\n",
       "Name: bug, Length: 420, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 3, 0, 0, 0, 2, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = Y_train.to_numpy()\n",
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.43555707,  0.00661348, -0.00150416, ...,  0.0810492 ,\n",
       "         -0.04502729, -0.04694391]],\n",
       "\n",
       "       [[-0.58595064,  0.25955952, -0.13917763, ..., -0.0635911 ,\n",
       "          0.00664355,  0.21386638]],\n",
       "\n",
       "       [[ 0.27632365, -0.50600654, -0.32239189, ..., -0.05089842,\n",
       "          0.05516215, -0.0075081 ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.27636816, -0.48560036,  0.76397859, ..., -0.03714568,\n",
       "         -0.1538287 ,  0.23404773]],\n",
       "\n",
       "       [[-0.02364086, -0.85175387,  0.47017971, ...,  0.43557536,\n",
       "         -0.43372011,  0.03525348]],\n",
       "\n",
       "       [[-0.30582691,  0.13031094, -0.51606078, ..., -0.0809641 ,\n",
       "         -0.00836604, -0.0343912 ]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "      ..\n",
       "173    0\n",
       "174    0\n",
       "175    0\n",
       "176    1\n",
       "177    0\n",
       "Name: bug, Length: 178, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "       0, 3, 0, 0, 0, 0, 3, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,\n",
       "       0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y = Y_test.to_numpy()\n",
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Designing and initializing the model.\n",
    "model = Sequential()\n",
    "model.add(GRU(100, input_shape = (1,components), dropout = 0.2, return_sequences=True))\n",
    "model.add(GRU(80, dropout = 0.2, return_sequences=True))\n",
    "model.add(GRU(60, dropout = 0.2, return_sequences=False))\n",
    "model.add(Dense(1, activation = 'relu'))\n",
    "model.compile(loss = 'mse' , optimizer = 'adam' , metrics = ['mse', 'mae', tf.keras.metrics.RootMeanSquaredError(), tf.keras.metrics.MeanSquaredLogarithmicError()] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru (GRU)                    (None, 1, 100)            33600     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 1, 80)             43680     \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 60)                25560     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 61        \n",
      "=================================================================\n",
      "Total params: 102,901\n",
      "Trainable params: 102,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.4458 - mse: 3.4458 - mae: 1.4902 - root_mean_squared_error: 1.8563 - mean_squared_logarithmic_error: 0.8743\n",
      "Epoch 2/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 3.2412 - mse: 3.2412 - mae: 1.4501 - root_mean_squared_error: 1.8003 - mean_squared_logarithmic_error: 0.7754\n",
      "Epoch 3/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 2.9711 - mse: 2.9711 - mae: 1.3958 - root_mean_squared_error: 1.7237 - mean_squared_logarithmic_error: 0.6605\n",
      "Epoch 4/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 2.6064 - mse: 2.6064 - mae: 1.3141 - root_mean_squared_error: 1.6144 - mean_squared_logarithmic_error: 0.5283\n",
      "Epoch 5/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.1704 - mse: 2.1704 - mae: 1.2005 - root_mean_squared_error: 1.4732 - mean_squared_logarithmic_error: 0.4016\n",
      "Epoch 6/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 1.6706 - mse: 1.6706 - mae: 1.0461 - root_mean_squared_error: 1.2925 - mean_squared_logarithmic_error: 0.2902\n",
      "Epoch 7/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 1.1816 - mse: 1.1816 - mae: 0.9192 - root_mean_squared_error: 1.0870 - mean_squared_logarithmic_error: 0.2211\n",
      "Epoch 8/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 1.0297 - mse: 1.0297 - mae: 0.8564 - root_mean_squared_error: 1.0148 - mean_squared_logarithmic_error: 0.2315\n",
      "Epoch 9/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 1.1637 - mse: 1.1637 - mae: 0.8661 - root_mean_squared_error: 1.0787 - mean_squared_logarithmic_error: 0.2616\n",
      "Epoch 10/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 1.1159 - mse: 1.1159 - mae: 0.8599 - root_mean_squared_error: 1.0564 - mean_squared_logarithmic_error: 0.2553\n",
      "Epoch 11/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.9492 - mse: 0.9492 - mae: 0.8258 - root_mean_squared_error: 0.9743 - mean_squared_logarithmic_error: 0.2136\n",
      "Epoch 12/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.9216 - mse: 0.9216 - mae: 0.8312 - root_mean_squared_error: 0.9600 - mean_squared_logarithmic_error: 0.2074\n",
      "Epoch 13/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.9526 - mse: 0.9526 - mae: 0.8409 - root_mean_squared_error: 0.9760 - mean_squared_logarithmic_error: 0.2103\n",
      "Epoch 14/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.9006 - mse: 0.9006 - mae: 0.8164 - root_mean_squared_error: 0.9490 - mean_squared_logarithmic_error: 0.2019\n",
      "Epoch 15/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.8812 - mse: 0.8812 - mae: 0.7999 - root_mean_squared_error: 0.9387 - mean_squared_logarithmic_error: 0.2073\n",
      "Epoch 16/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.9464 - mse: 0.9464 - mae: 0.8183 - root_mean_squared_error: 0.9728 - mean_squared_logarithmic_error: 0.2229\n",
      "Epoch 17/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.9042 - mse: 0.9042 - mae: 0.8107 - root_mean_squared_error: 0.9509 - mean_squared_logarithmic_error: 0.2147\n",
      "Epoch 18/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.9132 - mse: 0.9132 - mae: 0.8193 - root_mean_squared_error: 0.9556 - mean_squared_logarithmic_error: 0.2112\n",
      "Epoch 19/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.9447 - mse: 0.9447 - mae: 0.8357 - root_mean_squared_error: 0.9720 - mean_squared_logarithmic_error: 0.2149\n",
      "Epoch 20/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.8729 - mse: 0.8729 - mae: 0.8036 - root_mean_squared_error: 0.9343 - mean_squared_logarithmic_error: 0.1985\n",
      "Epoch 21/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.8815 - mse: 0.8815 - mae: 0.8030 - root_mean_squared_error: 0.9389 - mean_squared_logarithmic_error: 0.2010\n",
      "Epoch 22/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.8858 - mse: 0.8858 - mae: 0.7781 - root_mean_squared_error: 0.9412 - mean_squared_logarithmic_error: 0.2067\n",
      "Epoch 23/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.8708 - mse: 0.8708 - mae: 0.7814 - root_mean_squared_error: 0.9332 - mean_squared_logarithmic_error: 0.2029\n",
      "Epoch 24/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.8965 - mse: 0.8965 - mae: 0.7819 - root_mean_squared_error: 0.9468 - mean_squared_logarithmic_error: 0.2081\n",
      "Epoch 25/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.9113 - mse: 0.9113 - mae: 0.7948 - root_mean_squared_error: 0.9546 - mean_squared_logarithmic_error: 0.2042\n",
      "Epoch 26/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8355 - mse: 0.8355 - mae: 0.7646 - root_mean_squared_error: 0.9141 - mean_squared_logarithmic_error: 0.1909\n",
      "Epoch 27/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.9106 - mse: 0.9106 - mae: 0.8034 - root_mean_squared_error: 0.9542 - mean_squared_logarithmic_error: 0.2030\n",
      "Epoch 28/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.8694 - mse: 0.8694 - mae: 0.7936 - root_mean_squared_error: 0.9324 - mean_squared_logarithmic_error: 0.2010\n",
      "Epoch 29/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8876 - mse: 0.8876 - mae: 0.7955 - root_mean_squared_error: 0.9421 - mean_squared_logarithmic_error: 0.2083\n",
      "Epoch 30/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8469 - mse: 0.8469 - mae: 0.7696 - root_mean_squared_error: 0.9203 - mean_squared_logarithmic_error: 0.1988\n",
      "Epoch 31/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8772 - mse: 0.8772 - mae: 0.7909 - root_mean_squared_error: 0.9366 - mean_squared_logarithmic_error: 0.2019\n",
      "Epoch 32/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8950 - mse: 0.8950 - mae: 0.7978 - root_mean_squared_error: 0.9460 - mean_squared_logarithmic_error: 0.2003\n",
      "Epoch 33/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.8773 - mse: 0.8773 - mae: 0.7926 - root_mean_squared_error: 0.9367 - mean_squared_logarithmic_error: 0.1918\n",
      "Epoch 34/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8000 - mse: 0.8000 - mae: 0.7533 - root_mean_squared_error: 0.8944 - mean_squared_logarithmic_error: 0.1859\n",
      "Epoch 35/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8868 - mse: 0.8868 - mae: 0.7654 - root_mean_squared_error: 0.9417 - mean_squared_logarithmic_error: 0.2087\n",
      "Epoch 36/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8708 - mse: 0.8708 - mae: 0.7723 - root_mean_squared_error: 0.9332 - mean_squared_logarithmic_error: 0.2003\n",
      "Epoch 37/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8299 - mse: 0.8299 - mae: 0.7668 - root_mean_squared_error: 0.9110 - mean_squared_logarithmic_error: 0.1909\n",
      "Epoch 38/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8069 - mse: 0.8069 - mae: 0.7564 - root_mean_squared_error: 0.8983 - mean_squared_logarithmic_error: 0.1828\n",
      "Epoch 39/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8134 - mse: 0.8134 - mae: 0.7450 - root_mean_squared_error: 0.9019 - mean_squared_logarithmic_error: 0.1842\n",
      "Epoch 40/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8990 - mse: 0.8990 - mae: 0.7930 - root_mean_squared_error: 0.9481 - mean_squared_logarithmic_error: 0.2069\n",
      "Epoch 41/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8215 - mse: 0.8215 - mae: 0.7402 - root_mean_squared_error: 0.9064 - mean_squared_logarithmic_error: 0.1841\n",
      "Epoch 42/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.8769 - mse: 0.8769 - mae: 0.7584 - root_mean_squared_error: 0.9364 - mean_squared_logarithmic_error: 0.1977\n",
      "Epoch 43/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8377 - mse: 0.8377 - mae: 0.7601 - root_mean_squared_error: 0.9152 - mean_squared_logarithmic_error: 0.1886\n",
      "Epoch 44/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8888 - mse: 0.8888 - mae: 0.7773 - root_mean_squared_error: 0.9427 - mean_squared_logarithmic_error: 0.2007\n",
      "Epoch 45/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8399 - mse: 0.8399 - mae: 0.7694 - root_mean_squared_error: 0.9164 - mean_squared_logarithmic_error: 0.1872\n",
      "Epoch 46/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8263 - mse: 0.8263 - mae: 0.7574 - root_mean_squared_error: 0.9090 - mean_squared_logarithmic_error: 0.1815\n",
      "Epoch 47/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7798 - mse: 0.7798 - mae: 0.7394 - root_mean_squared_error: 0.8830 - mean_squared_logarithmic_error: 0.1800\n",
      "Epoch 48/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8484 - mse: 0.8484 - mae: 0.7645 - root_mean_squared_error: 0.9211 - mean_squared_logarithmic_error: 0.1980\n",
      "Epoch 49/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8507 - mse: 0.8507 - mae: 0.7637 - root_mean_squared_error: 0.9223 - mean_squared_logarithmic_error: 0.1952\n",
      "Epoch 50/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8355 - mse: 0.8355 - mae: 0.7602 - root_mean_squared_error: 0.9141 - mean_squared_logarithmic_error: 0.1881\n",
      "Epoch 51/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8142 - mse: 0.8142 - mae: 0.7480 - root_mean_squared_error: 0.9023 - mean_squared_logarithmic_error: 0.1838\n",
      "Epoch 52/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8293 - mse: 0.8293 - mae: 0.7653 - root_mean_squared_error: 0.9107 - mean_squared_logarithmic_error: 0.1881\n",
      "Epoch 53/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8623 - mse: 0.8623 - mae: 0.7845 - root_mean_squared_error: 0.9286 - mean_squared_logarithmic_error: 0.1916\n",
      "Epoch 54/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8227 - mse: 0.8227 - mae: 0.7514 - root_mean_squared_error: 0.9070 - mean_squared_logarithmic_error: 0.1855\n",
      "Epoch 55/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.8403 - mse: 0.8403 - mae: 0.7450 - root_mean_squared_error: 0.9167 - mean_squared_logarithmic_error: 0.1848\n",
      "Epoch 56/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.8520 - mse: 0.8520 - mae: 0.7522 - root_mean_squared_error: 0.9230 - mean_squared_logarithmic_error: 0.1957\n",
      "Epoch 57/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8429 - mse: 0.8429 - mae: 0.7552 - root_mean_squared_error: 0.9181 - mean_squared_logarithmic_error: 0.1960\n",
      "Epoch 58/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7959 - mse: 0.7959 - mae: 0.7292 - root_mean_squared_error: 0.8922 - mean_squared_logarithmic_error: 0.1833\n",
      "Epoch 59/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8079 - mse: 0.8079 - mae: 0.7426 - root_mean_squared_error: 0.8988 - mean_squared_logarithmic_error: 0.1812\n",
      "Epoch 60/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8479 - mse: 0.8479 - mae: 0.7740 - root_mean_squared_error: 0.9208 - mean_squared_logarithmic_error: 0.1866\n",
      "Epoch 61/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8092 - mse: 0.8092 - mae: 0.7525 - root_mean_squared_error: 0.8995 - mean_squared_logarithmic_error: 0.1842\n",
      "Epoch 62/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8297 - mse: 0.8297 - mae: 0.7598 - root_mean_squared_error: 0.9109 - mean_squared_logarithmic_error: 0.1888\n",
      "Epoch 63/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8277 - mse: 0.8277 - mae: 0.7400 - root_mean_squared_error: 0.9098 - mean_squared_logarithmic_error: 0.1913\n",
      "Epoch 64/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8421 - mse: 0.8421 - mae: 0.7399 - root_mean_squared_error: 0.9177 - mean_squared_logarithmic_error: 0.1909\n",
      "Epoch 65/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7893 - mse: 0.7893 - mae: 0.7282 - root_mean_squared_error: 0.8884 - mean_squared_logarithmic_error: 0.1755\n",
      "Epoch 66/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7810 - mse: 0.7810 - mae: 0.7364 - root_mean_squared_error: 0.8837 - mean_squared_logarithmic_error: 0.1801\n",
      "Epoch 67/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8310 - mse: 0.8310 - mae: 0.7634 - root_mean_squared_error: 0.9116 - mean_squared_logarithmic_error: 0.1915\n",
      "Epoch 68/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7654 - mse: 0.7654 - mae: 0.7159 - root_mean_squared_error: 0.8749 - mean_squared_logarithmic_error: 0.1714\n",
      "Epoch 69/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8123 - mse: 0.8123 - mae: 0.7503 - root_mean_squared_error: 0.9013 - mean_squared_logarithmic_error: 0.1866\n",
      "Epoch 70/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8008 - mse: 0.8008 - mae: 0.7253 - root_mean_squared_error: 0.8949 - mean_squared_logarithmic_error: 0.1850\n",
      "Epoch 71/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7444 - mse: 0.7444 - mae: 0.7017 - root_mean_squared_error: 0.8628 - mean_squared_logarithmic_error: 0.1808\n",
      "Epoch 72/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.7768 - mse: 0.7768 - mae: 0.7149 - root_mean_squared_error: 0.8814 - mean_squared_logarithmic_error: 0.1794\n",
      "Epoch 73/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8098 - mse: 0.8098 - mae: 0.7334 - root_mean_squared_error: 0.8999 - mean_squared_logarithmic_error: 0.1808\n",
      "Epoch 74/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.8194 - mse: 0.8194 - mae: 0.7567 - root_mean_squared_error: 0.9052 - mean_squared_logarithmic_error: 0.1829\n",
      "Epoch 75/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.7829 - mse: 0.7829 - mae: 0.7207 - root_mean_squared_error: 0.8848 - mean_squared_logarithmic_error: 0.1680\n",
      "Epoch 76/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7796 - mse: 0.7796 - mae: 0.7176 - root_mean_squared_error: 0.8829 - mean_squared_logarithmic_error: 0.1779\n",
      "Epoch 77/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7834 - mse: 0.7834 - mae: 0.7068 - root_mean_squared_error: 0.8851 - mean_squared_logarithmic_error: 0.1764\n",
      "Epoch 78/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7908 - mse: 0.7908 - mae: 0.7222 - root_mean_squared_error: 0.8892 - mean_squared_logarithmic_error: 0.1817\n",
      "Epoch 79/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7974 - mse: 0.7974 - mae: 0.7144 - root_mean_squared_error: 0.8929 - mean_squared_logarithmic_error: 0.1807\n",
      "Epoch 80/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7610 - mse: 0.7610 - mae: 0.7131 - root_mean_squared_error: 0.8724 - mean_squared_logarithmic_error: 0.1672\n",
      "Epoch 81/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7753 - mse: 0.7753 - mae: 0.7230 - root_mean_squared_error: 0.8805 - mean_squared_logarithmic_error: 0.1759\n",
      "Epoch 82/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7984 - mse: 0.7984 - mae: 0.7301 - root_mean_squared_error: 0.8935 - mean_squared_logarithmic_error: 0.1785\n",
      "Epoch 83/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8052 - mse: 0.8052 - mae: 0.7378 - root_mean_squared_error: 0.8973 - mean_squared_logarithmic_error: 0.1815\n",
      "Epoch 84/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7407 - mse: 0.7407 - mae: 0.7041 - root_mean_squared_error: 0.8606 - mean_squared_logarithmic_error: 0.1731\n",
      "Epoch 85/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7492 - mse: 0.7492 - mae: 0.6920 - root_mean_squared_error: 0.8656 - mean_squared_logarithmic_error: 0.1768\n",
      "Epoch 86/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7203 - mse: 0.7203 - mae: 0.6902 - root_mean_squared_error: 0.8487 - mean_squared_logarithmic_error: 0.1706\n",
      "Epoch 87/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7807 - mse: 0.7807 - mae: 0.7196 - root_mean_squared_error: 0.8836 - mean_squared_logarithmic_error: 0.1804\n",
      "Epoch 88/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7717 - mse: 0.7717 - mae: 0.7181 - root_mean_squared_error: 0.8785 - mean_squared_logarithmic_error: 0.1762\n",
      "Epoch 89/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7731 - mse: 0.7731 - mae: 0.7158 - root_mean_squared_error: 0.8792 - mean_squared_logarithmic_error: 0.1677\n",
      "Epoch 90/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7348 - mse: 0.7348 - mae: 0.6974 - root_mean_squared_error: 0.8572 - mean_squared_logarithmic_error: 0.1659\n",
      "Epoch 91/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7475 - mse: 0.7475 - mae: 0.6850 - root_mean_squared_error: 0.8646 - mean_squared_logarithmic_error: 0.1746\n",
      "Epoch 92/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7687 - mse: 0.7687 - mae: 0.7004 - root_mean_squared_error: 0.8768 - mean_squared_logarithmic_error: 0.1801\n",
      "Epoch 93/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.7569 - mse: 0.7569 - mae: 0.7003 - root_mean_squared_error: 0.8700 - mean_squared_logarithmic_error: 0.1764\n",
      "Epoch 94/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7214 - mse: 0.7214 - mae: 0.6845 - root_mean_squared_error: 0.8493 - mean_squared_logarithmic_error: 0.1643\n",
      "Epoch 95/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7498 - mse: 0.7498 - mae: 0.7094 - root_mean_squared_error: 0.8659 - mean_squared_logarithmic_error: 0.1635\n",
      "Epoch 96/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7561 - mse: 0.7561 - mae: 0.7008 - root_mean_squared_error: 0.8696 - mean_squared_logarithmic_error: 0.1749\n",
      "Epoch 97/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7230 - mse: 0.7230 - mae: 0.6716 - root_mean_squared_error: 0.8503 - mean_squared_logarithmic_error: 0.1707\n",
      "Epoch 98/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7273 - mse: 0.7273 - mae: 0.6554 - root_mean_squared_error: 0.8528 - mean_squared_logarithmic_error: 0.1758\n",
      "Epoch 99/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7561 - mse: 0.7561 - mae: 0.6959 - root_mean_squared_error: 0.8695 - mean_squared_logarithmic_error: 0.1724\n",
      "Epoch 100/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7483 - mse: 0.7483 - mae: 0.6987 - root_mean_squared_error: 0.8650 - mean_squared_logarithmic_error: 0.1648\n"
     ]
    }
   ],
   "source": [
    "# Fitting the model on training data.\n",
    "history = model.fit(train_x, train_y, epochs = 100, batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.2128844 ],\n",
       "       [2.1001894 ],\n",
       "       [0.5793765 ],\n",
       "       [1.365251  ],\n",
       "       [1.7998129 ],\n",
       "       [1.272622  ],\n",
       "       [1.4693505 ],\n",
       "       [0.        ],\n",
       "       [0.94844353],\n",
       "       [1.5052599 ],\n",
       "       [1.5790235 ],\n",
       "       [2.2282186 ],\n",
       "       [0.887205  ],\n",
       "       [1.1893156 ],\n",
       "       [1.9562536 ],\n",
       "       [0.2763195 ],\n",
       "       [1.0058887 ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.87801856],\n",
       "       [0.99994624],\n",
       "       [0.5997076 ],\n",
       "       [0.6850769 ],\n",
       "       [1.7849791 ],\n",
       "       [2.000596  ],\n",
       "       [1.791516  ],\n",
       "       [1.2346964 ],\n",
       "       [0.7122446 ],\n",
       "       [0.7315466 ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [1.4537289 ],\n",
       "       [0.43116868],\n",
       "       [0.        ],\n",
       "       [1.4030751 ],\n",
       "       [0.        ],\n",
       "       [0.81705755],\n",
       "       [0.6251728 ],\n",
       "       [1.5417311 ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.2870173 ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.90504277],\n",
       "       [1.6189055 ],\n",
       "       [2.830495  ],\n",
       "       [0.1102867 ],\n",
       "       [0.7575225 ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.10094247],\n",
       "       [0.        ],\n",
       "       [1.6096568 ],\n",
       "       [1.5131913 ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.7934808 ],\n",
       "       [0.81921583],\n",
       "       [1.6980168 ],\n",
       "       [0.        ],\n",
       "       [2.1299791 ],\n",
       "       [1.3849112 ],\n",
       "       [1.5172908 ],\n",
       "       [1.9557611 ],\n",
       "       [0.44271344],\n",
       "       [0.        ],\n",
       "       [1.9120182 ],\n",
       "       [0.        ],\n",
       "       [1.7686254 ],\n",
       "       [0.63279396],\n",
       "       [1.816075  ],\n",
       "       [2.138357  ],\n",
       "       [2.048828  ],\n",
       "       [0.7595685 ],\n",
       "       [1.4624484 ],\n",
       "       [1.2549819 ],\n",
       "       [2.744624  ],\n",
       "       [0.        ],\n",
       "       [1.1128936 ],\n",
       "       [1.7140986 ],\n",
       "       [0.41198522],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [1.3911533 ],\n",
       "       [1.4797913 ],\n",
       "       [1.6399497 ],\n",
       "       [1.4700564 ],\n",
       "       [0.11899761],\n",
       "       [1.593186  ],\n",
       "       [0.        ],\n",
       "       [1.6233917 ],\n",
       "       [0.9708505 ],\n",
       "       [0.        ],\n",
       "       [0.48650733],\n",
       "       [1.8079206 ],\n",
       "       [0.29020783],\n",
       "       [1.1862904 ],\n",
       "       [0.        ],\n",
       "       [0.4177529 ],\n",
       "       [0.7207244 ],\n",
       "       [0.81581753],\n",
       "       [2.1227186 ],\n",
       "       [0.51402867],\n",
       "       [0.        ],\n",
       "       [0.74283   ],\n",
       "       [2.4950402 ],\n",
       "       [0.24392554],\n",
       "       [1.495812  ],\n",
       "       [0.6061786 ],\n",
       "       [0.9677748 ],\n",
       "       [0.76879233],\n",
       "       [0.8575249 ],\n",
       "       [1.3687545 ],\n",
       "       [1.6550838 ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.84040105],\n",
       "       [0.13885131],\n",
       "       [1.4520472 ],\n",
       "       [1.8124481 ],\n",
       "       [0.        ],\n",
       "       [1.1222502 ],\n",
       "       [1.1499599 ],\n",
       "       [1.0550365 ],\n",
       "       [1.3161536 ],\n",
       "       [1.4953951 ],\n",
       "       [1.0623441 ],\n",
       "       [1.0281963 ],\n",
       "       [0.        ],\n",
       "       [1.2656839 ],\n",
       "       [0.83023953],\n",
       "       [1.0290545 ],\n",
       "       [0.12712583],\n",
       "       [1.3134049 ],\n",
       "       [1.4142543 ],\n",
       "       [1.7318598 ],\n",
       "       [0.        ],\n",
       "       [0.76119906],\n",
       "       [0.81774646],\n",
       "       [0.24600235],\n",
       "       [1.5001374 ],\n",
       "       [0.20169446],\n",
       "       [0.        ],\n",
       "       [1.027936  ],\n",
       "       [1.6590036 ],\n",
       "       [1.7135422 ],\n",
       "       [1.0443394 ],\n",
       "       [0.        ],\n",
       "       [1.1854739 ],\n",
       "       [0.        ],\n",
       "       [1.1378111 ],\n",
       "       [1.5471344 ],\n",
       "       [0.        ],\n",
       "       [1.6581916 ],\n",
       "       [1.4039236 ],\n",
       "       [1.72745   ],\n",
       "       [1.6558715 ],\n",
       "       [0.4299372 ],\n",
       "       [1.1189564 ],\n",
       "       [1.7870301 ],\n",
       "       [1.4795569 ],\n",
       "       [1.4081761 ],\n",
       "       [1.5749123 ],\n",
       "       [1.4209852 ],\n",
       "       [0.35906178],\n",
       "       [0.92438793],\n",
       "       [0.        ],\n",
       "       [1.7194833 ],\n",
       "       [0.        ],\n",
       "       [1.0663087 ],\n",
       "       [1.6821631 ],\n",
       "       [1.552013  ],\n",
       "       [1.0715743 ],\n",
       "       [0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the predictions.\n",
    "predictions_y = model.predict(test_x)\n",
    "predictions_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [3.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [0.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [2.],\n",
       "       [0.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [3.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [2.],\n",
       "       [0.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [2.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [2.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [0.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [2.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [0.]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rounding off the predictions to nearest\n",
    "#integer as count of bugs is an integer.\n",
    "predictions_y_round = np.rint(predictions_y)\n",
    "predictions_y_round "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Viewing the number of predictions.\n",
    "predictions_y_round.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([167.], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the sum of all the predictions obtained to used while obtaining FPA\n",
    "s = 0\n",
    "for  t in range(predictions_y_round.shape[0]):\n",
    "    s+=predictions_y_round[t]\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.49431464], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtaining the value of FPA metric for the model\n",
    "Fpa = 0\n",
    "for  t in range(predictions_y_round.shape[0]):\n",
    "        x = 0\n",
    "        for j in range( predictions_y_round.shape[0]-t+1, predictions_y_round.shape[0]):\n",
    "               x = x + predictions_y_round[j]\n",
    "        \n",
    "        x = (x/s)/predictions_y_round.shape[0]\n",
    "        Fpa = Fpa + x\n",
    "Fpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5027083], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtaining the value of CLC metric for the model\n",
    "previous_obtained = predictions_y_round[predictions_y_round.shape[0] - 1]/s\n",
    "\n",
    "CLC = 0\n",
    "for i in range(predictions_y_round.shape[0]):\n",
    "    if(i==0):\n",
    "        CLC += 0 + previous_obtained\n",
    "    else:\n",
    "        additional = (predictions_y_round[predictions_y_round.shape[0] - 1 - i])/s\n",
    "        CLC += 2*previous_obtained + additional\n",
    "        previous_obtained += additional\n",
    "        \n",
    "CLC/=(2*predictions_y_round.shape[0])\n",
    "CLC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 1ms/step - loss: 1.1861 - mse: 1.1861 - mae: 0.8551 - root_mean_squared_error: 1.0891 - mean_squared_logarithmic_error: 0.4216\n",
      "dict_keys(['loss', 'mse', 'mae', 'root_mean_squared_error', 'mean_squared_logarithmic_error'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['loss',\n",
       " 'mse',\n",
       " 'mae',\n",
       " 'root_mean_squared_error',\n",
       " 'mean_squared_logarithmic_error']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting direct metric results using the metrics given to model.\n",
    "score = model.evaluate(test_x, test_y)\n",
    "print(history.history.keys())\n",
    "model.test_on_batch(test_x, test_y)\n",
    "model.metrics_names\n",
    "#print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\n",
    "#print(\"\\n%s: %.2f%%\" % (model.metrics_names[2], score[2]*100))\n",
    "#print(\"\\n%s: %.2f%%\" % (model.metrics_names[3], score[3]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/bavanya/Downloads/deb_packages/home/bavanya/Desktop/venv_python/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /home/bavanya/Downloads/deb_packages/home/bavanya/Desktop/venv_python/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: /home/bavanya/Desktop/6thSem/BTP/regression_PROMISE_dataset/saved_models/ant1.3_ant1.4_model5/ant1.3_ant1.4_model5_4/assets\n"
     ]
    }
   ],
   "source": [
    "# Saving the model\n",
    "model_id = 25\n",
    "path_to_save = '/home/bavanya/Desktop/6thSem/BTP/regression_PROMISE_dataset/saved_models/ant1.3_ant1.4_model5/ant1.3_ant1.4_model5_4'\n",
    "model.save(path_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the results to csv file.\n",
    "heading = ['model_id', 'train_data_name', 'test_data_name'] + model.metrics_names + ['fpa', 'clc']\n",
    "score = [model_id, train_data_name, test_data_name] + score + [float(Fpa) , float(CLC)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_id',\n",
       " 'train_data_name',\n",
       " 'test_data_name',\n",
       " 'loss',\n",
       " 'mse',\n",
       " 'mae',\n",
       " 'root_mean_squared_error',\n",
       " 'mean_squared_logarithmic_error',\n",
       " 'fpa',\n",
       " 'clc']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25,\n",
       " 'ant-1.3',\n",
       " 'ant-1.4',\n",
       " 1.1860753297805786,\n",
       " 1.1860753297805786,\n",
       " 0.8550759553909302,\n",
       " 1.0890707969665527,\n",
       " 0.42155712842941284,\n",
       " 0.49431464076042175,\n",
       " 0.5027083158493042]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the results to csv file.\n",
    "with open(path_to_save + '_metric_results.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(heading)\n",
    "    writer.writerow(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model and to make sure that the model is saved properly.\n",
    "model_loaded = load_model(path_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
